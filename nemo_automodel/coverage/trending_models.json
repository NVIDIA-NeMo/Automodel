[
    {
        "trending_rank": 2,
        "model_id": "ibm-granite/granite-4.0-micro",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 6,
        "model_id": "ibm-granite/granite-4.0-h-tiny",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 7,
        "model_id": "ibm-granite/granite-4.0-h-micro",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 9,
        "model_id": "ai21labs/AI21-Jamba-Reasoning-3B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 11,
        "model_id": "Qwen/Qwen3-0.6B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 13,
        "model_id": "ibm-granite/granite-4.0-micro-base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 15,
        "model_id": "meta-llama/Llama-3.1-8B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 16,
        "model_id": "ibm-granite/granite-4.0-h-micro-base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 17,
        "model_id": "TildeAI/TildeOpen-30b",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 19,
        "model_id": "Qwen/Qwen3-4B-Instruct-2507",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 20,
        "model_id": "ibm-granite/granite-4.0-h-tiny-base",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 21,
        "model_id": "meta-llama/Llama-3.1-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 22,
        "model_id": "ibm-granite/granite-4.0-tiny-preview",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 24,
        "model_id": "Kwaipilot/KAT-Dev",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 31,
        "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 33,
        "model_id": "meta-llama/Llama-3.2-1B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 37,
        "model_id": "LiquidAI/LFM2-2.6B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 38,
        "model_id": "LiquidAI/LFM2-1.2B-Extract",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 40,
        "model_id": "openai-community/gpt2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openai-community/gpt2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openai-community/gpt2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 41,
        "model_id": "Qwen/Qwen3-4B-Thinking-2507",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 42,
        "model_id": "google/gemma-3-270m",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-270m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-270m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 45,
        "model_id": "meta-llama/Llama-3.2-1B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 46,
        "model_id": "meta-llama/Llama-3.2-3B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 47,
        "model_id": "meta-llama/Llama-3.3-70B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 49,
        "model_id": "meta-llama/Llama-2-7b-chat-hf",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 50,
        "model_id": "google/gemma-3-1b-it",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 51,
        "model_id": "LiquidAI/LFM2-350M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 53,
        "model_id": "LiquidAI/LFM2-1.2B-Tool",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 55,
        "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 56,
        "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 57,
        "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 59,
        "model_id": "FractalAIResearch/Fathom-Search-4B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 60,
        "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 62,
        "model_id": "Qwen/Qwen2.5-7B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 63,
        "model_id": "Qwen/Qwen3-4B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 64,
        "model_id": "Qwen/Qwen3-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 65,
        "model_id": "ibm-granite/granite-4.0-tiny-base-preview",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-base-preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 66,
        "model_id": "ERC-ITEA/MuduoLLM",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 69,
        "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 72,
        "model_id": "LLM360/K2-Think",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "LLM360/K2-Think",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LLM360/K2-Think",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 76,
        "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 77,
        "model_id": "meta-llama/Meta-Llama-3-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 79,
        "model_id": "Qwen/Qwen2.5-3B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 80,
        "model_id": "Qwen/Qwen3-1.7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 81,
        "model_id": "suayptalha/Sungur-9B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 82,
        "model_id": "HuggingFaceTB/SmolLM3-3B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 83,
        "model_id": "LiquidAI/LFM2-1.2B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 84,
        "model_id": "NousResearch/Hermes-4-70B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 87,
        "model_id": "LiquidAI/LFM2-1.2B-RAG",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 90,
        "model_id": "microsoft/UserLM-8b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 91,
        "model_id": "Tesslate/UIGEN-FX-Agentic-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 95,
        "model_id": "FlareRebellion/WeirdCompound-v1.6-24b",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 98,
        "model_id": "LiquidAI/LFM2-350M-Extract",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 99,
        "model_id": "inclusionAI/Ring-mini-2.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 4,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 102,
        "model_id": "Shekswess/trlm-135m",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 103,
        "model_id": "foreverlasting1202/QuestA-Nemotron-1.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 104,
        "model_id": "fangwu97/DeepSearch-1.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 105,
        "model_id": "Qwen/Qwen3-4B-SafeRL",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 107,
        "model_id": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 108,
        "model_id": "meta-llama/Llama-2-7b-hf",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 109,
        "model_id": "mistralai/Mistral-7B-v0.1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 110,
        "model_id": "google/gemma-2b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-2b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-2b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 111,
        "model_id": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 112,
        "model_id": "Qwen/Qwen2.5-0.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 113,
        "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 114,
        "model_id": "microsoft/phi-4",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/phi-4",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 115,
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 116,
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 118,
        "model_id": "Qwen/Qwen3-32B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 119,
        "model_id": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 120,
        "model_id": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 125,
        "model_id": "google/gemma-3-270m-it",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 129,
        "model_id": "LiquidAI/LFM2-350M-Math",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 133,
        "model_id": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 135,
        "model_id": "Vortex5/MN-14B-Crimson-Veil",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 136,
        "model_id": "DreadPoor/Famino-12B-Model_Stock",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 138,
        "model_id": "AgentFlow/agentflow-planner-7b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 139,
        "model_id": "ByteDance-Seed/BFS-Prover-V2-7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 140,
        "model_id": "distilbert/distilgpt2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "distilbert/distilgpt2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "distilbert/distilgpt2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 142,
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 143,
        "model_id": "dphn/dolphin-2.9-llama3-8b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 145,
        "model_id": "mlabonne/NeuralDaredevil-8B-abliterated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 146,
        "model_id": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 147,
        "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 64,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 148,
        "model_id": "HuggingFaceTB/SmolLM2-135M-Instruct",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 149,
        "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 256,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 128,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 150,
        "model_id": "m-a-p/YuE-s1-7B-anneal-en-cot",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 153,
        "model_id": "viettelsecurity-ai/security-llama3.2-3b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 156,
        "model_id": "yamatazen/Gemma2-Snowflakes-9B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 157,
        "model_id": "Qwen/Qwen3-14B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 159,
        "model_id": "allenai/OLMo-2-0425-1B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 160,
        "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 161,
        "model_id": "yamatazen/FusionEngine-12B-Lorablated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 162,
        "model_id": "moonshotai/Kimi-Dev-72B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 164,
        "model_id": "Goedel-LM/Goedel-Prover-V2-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 172,
        "model_id": "apple/FastVLM-0.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 173,
        "model_id": "driaforall/mem-agent",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "driaforall/mem-agent",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "driaforall/mem-agent",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 175,
        "model_id": "inclusionAI/Ling-mini-2.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 4,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 178,
        "model_id": "GeneralAnalysis/GA_Guard_Core",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Core",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Core",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 179,
        "model_id": "Pinkstack/DistilGPT-OSS-qwen3-4B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 181,
        "model_id": "Qwen/Qwen3Guard-Gen-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 185,
        "model_id": "Tesslate/WEBGEN-Devstral-24B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 192,
        "model_id": "unsloth/granite-4.0-h-tiny",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-h-tiny",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 196,
        "model_id": "Vortex5/MN-12B-Azure-Veil",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 197,
        "model_id": "Jackrong/gpt-oss-120b-Distill-Llama3.1-8B-v2",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 8,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Jackrong/gpt-oss-120b-Distill-Llama3.1-8B-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 8,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 198,
        "model_id": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 199,
        "model_id": "huihui-ai/Huihui-granite-4.0-h-tiny-abliterated",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-tiny-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 200,
        "model_id": "yasserrmd/kallamni-4b-v1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 201,
        "model_id": "taozi555/MN-12B-Mag-Mell-R1-KTO",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 202,
        "model_id": "openai-community/gpt2-xl",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openai-community/gpt2-xl",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 203,
        "model_id": "microsoft/DialoGPT-medium",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/DialoGPT-medium",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 205,
        "model_id": "QuixiAI/WizardLM-13B-Uncensored",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 207,
        "model_id": "JackFram/llama-68m",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "JackFram/llama-68m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 208,
        "model_id": "Gryphe/MythoMax-L2-13b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 209,
        "model_id": "codellama/CodeLlama-34b-hf",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 210,
        "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 212,
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 213,
        "model_id": "ZySec-AI/SecurityLLM",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 214,
        "model_id": "google/gemma-7b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 216,
        "model_id": "aaditya/Llama3-OpenBioLLM-70B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "aaditya/Llama3-OpenBioLLM-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 217,
        "model_id": "dphn/dolphin-2.9.2-qwen2-7b",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 219,
        "model_id": "meta-llama/Llama-3.1-70B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 220,
        "model_id": "meta-llama/Llama-3.1-70B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 221,
        "model_id": "NousResearch/Hermes-3-Llama-3.1-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 222,
        "model_id": "utter-project/EuroLLM-1.7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 225,
        "model_id": "Qwen/Qwen2.5-7B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 226,
        "model_id": "Qwen/Qwen2.5-72B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 227,
        "model_id": "Qwen/Qwen2.5-32B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 228,
        "model_id": "Qwen/Qwen2.5-Coder-1.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 229,
        "model_id": "meta-llama/Llama-3.2-3B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 232,
        "model_id": "arcee-ai/Arcee-VyLinh",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 234,
        "model_id": "infly/OpenCoder-8B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 235,
        "model_id": "yentinglin/Llama-3.1-Taiwan-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 237,
        "model_id": "CohereLabs/c4ai-command-r7b-12-2024",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 238,
        "model_id": "jinaai/ReaderLM-v2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 239,
        "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 240,
        "model_id": "sometimesanotion/Lamarck-14B-v0.7",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "sometimesanotion/Lamarck-14B-v0.7",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 241,
        "model_id": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 242,
        "model_id": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 243,
        "model_id": "microsoft/MediPhi",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/MediPhi",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/MediPhi",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 244,
        "model_id": "humain-ai/ALLaM-7B-Instruct-preview",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 245,
        "model_id": "google/gemma-3-1b-pt",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 250,
        "model_id": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 251,
        "model_id": "Salesforce/xLAM-2-3b-fc-r",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 252,
        "model_id": "Salesforce/xLAM-2-1b-fc-r",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 254,
        "model_id": "ibm-granite/granite-3.3-2b-instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 256,
        "model_id": "DeepHat/DeepHat-V1-7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 257,
        "model_id": "Qwen/Qwen3-8B-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 258,
        "model_id": "Qwen/Qwen3-1.7B-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 259,
        "model_id": "yamatazen/SnowElf-12B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 261,
        "model_id": "microsoft/Phi-4-mini-reasoning",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 262,
        "model_id": "mlabonne/Qwen3-14B-abliterated",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 265,
        "model_id": "Gensyn/Qwen2.5-7B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 267,
        "model_id": "darkc0de/XortronCriminalComputingConfig",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 268,
        "model_id": "tiiuae/Falcon-H1-7B-Instruct",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "tiiuae/Falcon-H1-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 269,
        "model_id": "darkc0de/BlackXorDolphTronGOAT",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 271,
        "model_id": "yamatazen/NeonMaid-12B-v2",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/NeonMaid-12B-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 272,
        "model_id": "yamatazen/EsotericSage-12B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 273,
        "model_id": "Menlo/Jan-nano",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Menlo/Jan-nano",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Menlo/Jan-nano",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 274,
        "model_id": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 275,
        "model_id": "kyx0r/Neona-12B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 276,
        "model_id": "agentica-org/DeepSWE-Preview",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 278,
        "model_id": "gustavecortal/Piaget-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 279,
        "model_id": "LiquidAI/LFM2-700M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-700M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LiquidAI/LFM2-700M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 280,
        "model_id": "LGAI-EXAONE/EXAONE-4.0-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 284,
        "model_id": "zerofata/MS3.2-PaintedFantasy-v2-24B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 291,
        "model_id": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 292,
        "model_id": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 293,
        "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 296,
        "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 297,
        "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 298,
        "model_id": "kreasof-ai/Liquid-Thinking-Preview",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kreasof-ai/Liquid-Thinking-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kreasof-ai/Liquid-Thinking-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 300,
        "model_id": "Tesslate/UIGEN-FX-4B-Preview",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 303,
        "model_id": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 304,
        "model_id": "NousResearch/Hermes-4-14B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 305,
        "model_id": "OddTheGreat/Circuitry_24B_V.2",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 308,
        "model_id": "Benyucong/rl_quantum_4b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 312,
        "model_id": "GeneralAnalysis/GA_Guard_Lite",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Lite",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Lite",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 313,
        "model_id": "beyoru/Lunaa",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "beyoru/Lunaa",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "beyoru/Lunaa",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 315,
        "model_id": "Qwen/Qwen3Guard-Gen-0.6B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 316,
        "model_id": "suayptalha/Sungur-14B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 319,
        "model_id": "haoranhe/ROVER-Qwen3-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 321,
        "model_id": "dongboklee/gPRM-14B-merged",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 322,
        "model_id": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 323,
        "model_id": "Vortex5/Harmonic-Moon-12B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vortex5/Harmonic-Moon-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 324,
        "model_id": "ByteDance-Seed/BFS-Prover-V2-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 329,
        "model_id": "AnuroopVJ/Adheren-phi4-finetuned",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 8,
                    "sequence_parallel": false,
                    "tp_size": 2
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 8,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 331,
        "model_id": "unsloth/granite-4.0-micro",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 334,
        "model_id": "unsloth/granite-4.0-h-micro",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-h-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-h-micro",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 335,
        "model_id": "unsloth/granite-4.0-micro-base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "unsloth/granite-4.0-micro-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 339,
        "model_id": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 341,
        "model_id": "purrgpt-community/Tiny-Purr-350M",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "purrgpt-community/Tiny-Purr-350M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 342,
        "model_id": "AgentFlow/agentflow-planner-3b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 343,
        "model_id": "Clemylia/Malya",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Clemylia/Malya",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Clemylia/Malya",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 345,
        "model_id": "hoangtung386/TinyLlama-1.1B-qlora",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "hoangtung386/TinyLlama-1.1B-qlora",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "hoangtung386/TinyLlama-1.1B-qlora",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 346,
        "model_id": "Vortex5/Noir-Blossom-12B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vortex5/Noir-Blossom-12B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 350,
        "model_id": "Retreatcost/KansenSakura-Erosion-RP-12b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 352,
        "model_id": "openai-community/gpt2-large",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openai-community/gpt2-large",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 356,
        "model_id": "EleutherAI/gpt-neo-125m",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "EleutherAI/gpt-neo-125m",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 357,
        "model_id": "gabtan99/dialogpt-tagalog-medium-10",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gabtan99/dialogpt-tagalog-medium-10",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gabtan99/dialogpt-tagalog-medium-10",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 359,
        "model_id": "huggingtweets/porngum_ebooks",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huggingtweets/porngum_ebooks",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 368,
        "model_id": "PygmalionAI/pygmalion-6b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 369,
        "model_id": "huggyllama/llama-7b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huggyllama/llama-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "huggyllama/llama-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 373,
        "model_id": "bigcode/starcoder",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "bigcode/starcoder",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 374,
        "model_id": "tiiuae/falcon-7b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "tiiuae/falcon-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 375,
        "model_id": "tiiuae/falcon-7b-instruct",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 377,
        "model_id": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 378,
        "model_id": "roneneldan/TinyStories-1M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 379,
        "model_id": "roneneldan/TinyStories-1Layer-21M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 384,
        "model_id": "NousResearch/Nous-Hermes-13b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-13b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 386,
        "model_id": "CobraMamba/mamba-gpt-3b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 387,
        "model_id": "cateto/korean-gpt-neox-125M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 388,
        "model_id": "lmsys/vicuna-7b-v1.3",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 391,
        "model_id": "openlm-research/open_llama_7b_v2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 392,
        "model_id": "Maykeye/TinyLLama-v0",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Maykeye/TinyLLama-v0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 394,
        "model_id": "meta-llama/Llama-2-13b-hf",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 395,
        "model_id": "openlm-research/open_llama_3b_v2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 397,
        "model_id": "georgesung/llama2_7b_chat_uncensored",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 399,
        "model_id": "lmsys/vicuna-7b-v1.5",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 400,
        "model_id": "lmsys/vicuna-13b-v1.5-16k",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 401,
        "model_id": "Universal-NER/UniNER-7B-all",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 405,
        "model_id": "codellama/CodeLlama-7b-hf",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 406,
        "model_id": "codellama/CodeLlama-7b-Instruct-hf",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 407,
        "model_id": "codellama/CodeLlama-34b-Instruct-hf",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 409,
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 410,
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 412,
        "model_id": "nickypro/tinyllama-15M",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 413,
        "model_id": "AdaptLLM/law-LLM",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/law-LLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/law-LLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 414,
        "model_id": "klyang/MentaLLaMA-chat-7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 416,
        "model_id": "mychen76/mistral7b_ocr_to_json_v1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 417,
        "model_id": "teknium/Mistral-Trismegistus-7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 420,
        "model_id": "HuggingFaceH4/zephyr-7b-beta",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 421,
        "model_id": "deepseek-ai/deepseek-coder-33b-base",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 422,
        "model_id": "openchat/openchat_3.5",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "openchat/openchat_3.5",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 423,
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 424,
        "model_id": "01-ai/Yi-34B-200K",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 425,
        "model_id": "01-ai/Yi-6B-200K",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 430,
        "model_id": "berkeley-nest/Starling-LM-7B-alpha",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "berkeley-nest/Starling-LM-7B-alpha",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 433,
        "model_id": "bigcode/starcoder2-3b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 435,
        "model_id": "AdaptLLM/finance-chat",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 436,
        "model_id": "AdaptLLM/law-chat",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/law-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "AdaptLLM/law-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 437,
        "model_id": "cosimoiaia/Loquace-Wizard-13B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 438,
        "model_id": "upstage/SOLAR-10.7B-Instruct-v1.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 439,
        "model_id": "upstage/SOLAR-10.7B-v1.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 440,
        "model_id": "Hypersniper/The_Philosopher_Zephyr_7B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 441,
        "model_id": "dphn/dolphin-2.5-mixtral-8x7b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.5-mixtral-8x7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 442,
        "model_id": "TriadParty/deepsex-6b-chat",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 443,
        "model_id": "simecek/cswikimistral_0.1",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "simecek/cswikimistral_0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 445,
        "model_id": "dphn/dolphin-2_6-phi-2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 446,
        "model_id": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 447,
        "model_id": "jeiku/Rosa_NSFW_Niche_3B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "jeiku/Rosa_NSFW_Niche_3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "jeiku/Rosa_NSFW_Niche_3B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 448,
        "model_id": "peiyi9979/math-shepherd-mistral-7b-prm",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 449,
        "model_id": "stabilityai/stable-code-3b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 452,
        "model_id": "Qwen/Qwen1.5-0.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 455,
        "model_id": "tokyotech-llm/Swallow-MS-7b-v0.1",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "tokyotech-llm/Swallow-MS-7b-v0.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 458,
        "model_id": "cais/HarmBench-Llama-2-13b-cls",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 461,
        "model_id": "prometheus-eval/prometheus-7b-v2.0",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 462,
        "model_id": "bigcode/starcoder2-7b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 463,
        "model_id": "gordicaleksa/YugoGPT",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 464,
        "model_id": "Lishi0905/SoMeLVLM",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 472,
        "model_id": "CohereLabs/c4ai-command-r-v01",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 477,
        "model_id": "google/codegemma-2b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/codegemma-2b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/codegemma-2b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 478,
        "model_id": "google/codegemma-7b",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "google/codegemma-7b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 484,
        "model_id": "Nitral-AI/Nyanade_Stunna-Maid-7B",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Nitral-AI/Nyanade_Stunna-Maid-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 486,
        "model_id": "Qwen/CodeQwen1.5-7B-Chat",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 487,
        "model_id": "nickmalhotra/ProjectIndus",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nickmalhotra/ProjectIndus",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nickmalhotra/ProjectIndus",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 489,
        "model_id": "cgato/L3-TheSpice-8b-v0.1.3",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 490,
        "model_id": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 491,
        "model_id": "Vanessasml/cyber-risk-llama-3-8b",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 492,
        "model_id": "kuotient/Llama-3-6B-Instruct-pruned",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 495,
        "model_id": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 497,
        "model_id": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 498,
        "model_id": "dphn/dolphin-2.9-llama3-8b-256k",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b-256k",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 499,
        "model_id": "BramVanroy/fietje-2-chat",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "BramVanroy/fietje-2-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "BramVanroy/fietje-2-chat",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 500,
        "model_id": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 503,
        "model_id": "BAAI/Bunny-v1_0-4B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 507,
        "model_id": "JetBrains/CodeLlama-7B-KStack",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 508,
        "model_id": "dphn/dolphin-2.9.1-yi-1.5-34b",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 509,
        "model_id": "CohereLabs/aya-23-8B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 513,
        "model_id": "yentinglin/Llama-3-Taiwan-70B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "yentinglin/Llama-3-Taiwan-70B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 514,
        "model_id": "chincyk/PyCodeGen",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 515,
        "model_id": "Qwen/Qwen2-0.5B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 517,
        "model_id": "Qwen/Qwen2-7B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 518,
        "model_id": "Sao10K/L3-8B-Stheno-v3.2",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 519,
        "model_id": "bosonai/Higgs-Llama-3-70B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "bosonai/Higgs-Llama-3-70B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 520,
        "model_id": "mii-llm/maestrale-chat-v0.4-beta",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "mii-llm/maestrale-chat-v0.4-beta",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 521,
        "model_id": "ajn313/cl-verilog-1.0",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 523,
        "model_id": "rubra-ai/Qwen2-7B-Instruct",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 524,
        "model_id": "nvidia/OpenReasoning-Nemotron-1.5B",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 525,
        "model_id": "nvidia/OpenReasoning-Nemotron-7B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 4
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 526,
        "model_id": "nvidia/OpenReasoning-Nemotron-14B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 527,
        "model_id": "nvidia/OpenReasoning-Nemotron-32B",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 2,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 2,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 529,
        "model_id": "nvidia/Nemotron-H-8B-Reasoning-128K",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 532,
        "model_id": "nvidia/Nemotron-H-8B-Base-8K",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 533,
        "model_id": "nvidia/Nemotron-H-4B-Base-8K",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Base-8K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 534,
        "model_id": "nvidia/Nemotron-H-4B-Instruct-128K",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 537,
        "model_id": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 538,
        "model_id": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    },
    {
        "trending_rank": 539,
        "model_id": "nvidia/Llama-3.3-Nemotron-70B-Edit",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 540,
        "model_id": "nvidia/Llama-3.3-Nemotron-70B-Select",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 541,
        "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 542,
        "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 543,
        "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 544,
        "model_id": "nvidia/Mistral-NeMo-Minitron-8B-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 545,
        "model_id": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 547,
        "model_id": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 548,
        "model_id": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 549,
        "model_id": "nvidia/Nemotron-Mini-4B-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 550,
        "model_id": "nvidia/Minitron-4B-Base",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 551,
        "model_id": "nvidia/Minitron-8B-Base",
        "sft": {},
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Minitron-8B-Base",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 553,
        "model_id": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 8,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 554,
        "model_id": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 555,
        "model_id": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
        "sft": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 1
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 0,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 561,
        "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {
            "config": {
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 2,
                    "dp_size": null,
                    "ep_size": 1,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 1,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        }
    },
    {
        "trending_rank": 562,
        "model_id": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
        "sft": {
            "config": {
                "autopipeline": {
                    "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                    "pp_microbatch_size": 1,
                    "pp_schedule": "interleaved1f1b",
                    "scale_grads_in_schedule": false
                },
                "distributed": {
                    "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                    "activation_checkpointing": true,
                    "cp_size": 1,
                    "dp_size": null,
                    "ep_size": 1,
                    "pp_size": 4,
                    "sequence_parallel": false,
                    "tp_size": 8
                },
                "loss_fn": {
                    "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                },
                "lr_scheduler": {
                    "lr_decay_style": "cosine",
                    "min_lr": 1e-06
                },
                "model": {
                    "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                    "attn_implementation": "eager",
                    "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                    "torch_dtype": "bf16",
                    "trust_remote_code": true,
                    "use_cache": false,
                    "use_sdpa_patching": false
                },
                "optimizer": {
                    "_target_": "torch.optim.adam.Adam",
                    "betas": [
                        0.9,
                        0.999
                    ],
                    "eps": 1e-08,
                    "lr": 1e-05,
                    "weight_decay": 0
                },
                "packed_sequence": {
                    "packed_sequence_size": 4096,
                    "split_across_pack": false
                },
                "step_scheduler": {
                    "ckpt_every_steps": 50,
                    "global_batch_size": 16,
                    "local_batch_size": 4,
                    "max_steps": 100,
                    "val_every_steps": 2000
                }
            },
            "commit_hash": "f06e8b5f02eda7a89b6942ac46be0935d07f9d12"
        },
        "lora": {}
    }
]