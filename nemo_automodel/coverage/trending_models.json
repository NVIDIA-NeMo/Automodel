{
    "spec_version": "0.1.0",
    "trendinglist_date": "20251022",
    "automodel_commit": "249b512a1fbf7383d351c258124b424dd6e7ddf8",
    "models": [
        {
            "model_id": "inclusionAI/Ling-1T",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ring-1T",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/UserLM-8b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "vandijklab/C2S-Scale-Gemma-2-27B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vandijklab/C2S-Scale-Gemma-2-27B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vandijklab/C2S-Scale-Gemma-2-27B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vandijklab/C2S-Scale-Gemma-2-27B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "zai-org/GLM-4.6",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "facebook/MobileLLM-Pro",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "KORMo-Team/KORMo-10B-sft",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openai/gpt-oss-20b",
            "sft": {
                "fsdp": {
                    "is_supported": null,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openai/gpt-oss-20b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openai/gpt-oss-20b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "openai/gpt-oss-20b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "katanemo/Arch-Router-1.5B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "katanemo/Arch-Router-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "katanemo/Arch-Router-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "katanemo/Arch-Router-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "katanemo/Arch-Router-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FractalAIResearch/Fathom-Search-4B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inference-net/Schematron-3B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-8B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openai/gpt-oss-120b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M-PII-Extract-JP",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-PII-Extract-JP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-PII-Extract-JP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-8B-A1B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "vngrs-ai/Kumru-2B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/LLaDA2.0-mini-preview",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai21labs/AI21-Jamba-Reasoning-3B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Kwaipilot/KAT-Dev-72B-Exp",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev-72B-Exp",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openbmb/MiniCPM4.1-8B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "KORMo-Team/KORMo-10B-base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-Instruct-2507",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LLM360/K2-Think",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LLM360/K2-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LLM360/K2-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LLM360/K2-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "radicalnumerics/RND1-Base-0910",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-0.6B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "moonshotai/Kimi-K2-Instruct-0905",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-3-1b-it",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-3-270m",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-270m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-270m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "sdobson/nanochat",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-1B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B-Instruct-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-Thinking-2507",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FractalAIResearch/Fathom-Synthesizer-4B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Synthesizer-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Synthesizer-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Synthesizer-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Synthesizer-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-SafeRL",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-3B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-1.7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-3n-E4B-it-litert-lm",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "swiss-ai/Apertus-8B-Instruct-2509",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-3-mini-4k-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Kwaipilot/KAT-Dev",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.3-70B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-3n-E2B-it-litert-lm",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-3-270m-it",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inference-net/Schematron-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "inference-net/Schematron-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inference-net/Schematron-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-tiny",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.1-Terminus",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openai-community/gpt2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "zai-org/GLM-4.5-Air",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meituan-longcat/LongCat-Flash-Chat",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-small",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-small",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FlareRebellion/WeirdCompound-v1.7-24b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.7-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.7-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.7-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.7-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Meta-Llama-3-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-2-2b-it",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-1B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "litert-community/Gemma3-1B-IT",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-32B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-3.3-8b-lora-math-prm",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-lora-math-prm",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM3-3B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ByteDance-Seed/Seed-OSS-36B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.1",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ling-mini-2.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "gustavecortal/Beck-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gustavecortal/Beck-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "gustavecortal/Beck-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gustavecortal/Beck-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gustavecortal/Beck-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gustavecortal/Beck-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-v0.1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-7B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "zai-org/GLM-4.5",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Fortytwo-Network/Strand-Rust-Coder-14B-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-Guard-3-8B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "jinaai/ReaderLM-v2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "BlinkDL/rwkv7-g1",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/bitnet-b1.58-2B-4T",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "moonshotai/Kimi-K2-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TildeAI/TildeOpen-30b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "swiss-ai/Apertus-70B-2509",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-micro",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ring-1T-preview",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "RESMP-DEV/Qwen3-Next-80B-A3B-Thinking-NVFP4",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "s3nh/EduHelp-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "s3nh/EduHelp-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "s3nh/EduHelp-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "s3nh/EduHelp-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "s3nh/EduHelp-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "s3nh/EduHelp-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-2-2b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "vngrs-ai/Kumru-2B-Base",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "vngrs-ai/Kumru-2B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-3.3-8b-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "fdtn-ai/Foundation-Sec-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "fdtn-ai/Foundation-Sec-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "fdtn-ai/Foundation-Sec-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "fdtn-ai/Foundation-Sec-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "fdtn-ai/Foundation-Sec-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "fdtn-ai/Foundation-Sec-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-14B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "darkc0de/XortronCriminalComputingConfig",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "baidu/ERNIE-4.5-21B-A3B-Thinking",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-micro",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meituan-longcat/LongCat-Flash-Thinking",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Salesforce/CoDA-v0-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Huihui-GLM-4.5-Air-abliterated-mlx-mxfp4",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Tesslate/UIGENT-30B-3A-Preview",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "vanta-research/apollo-astralis-8b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "vandijklab/C2S-Scale-Gemma-2-2B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/MS3.2-24B-Fiery-Lynx",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "dphn/Dolphin-X1-8B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/Dolphin-X1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/Scarlet-Ink-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Scarlet-Ink-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Vortex5/Scarlet-Ink-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Scarlet-Ink-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/Abyssal-Seraph-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Abyssal-Seraph-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Vortex5/Abyssal-Seraph-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Abyssal-Seraph-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "distilbert/distilgpt2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "EleutherAI/gpt-neo-125m",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-neo-125m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "bigscience/bloom",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/ruGPT-3.5-13B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/ruGPT-3.5-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/ruGPT-3.5-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-llm-7b-chat",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-2b-it",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-0.5B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "kyungbae/gemma-2-finance",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HumanLLMs/Human-Like-Mistral-Nemo-Instruct-2407",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/phi-4",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/phi-4",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "microsoft/phi-4",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/phi-4",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "perplexity-ai/r1-1776",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/QwQ-32B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/QwQ-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/QwQ-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/QwQ-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "DeepHat/DeepHat-V1-7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-0.6B-Base",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-0528",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "OmniSVG/OmniSVG",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepcogito/cogito-v2-preview-llama-405B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
            "sft": {
                "fsdp": {
                    "is_supported": null,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "onnx-community/gemma-3-270m-it-ONNX",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "NousResearch/Hermes-4-70B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-Extract",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder-V1.2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder-V1.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder-V1.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder-V1.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-MOE-4x0.6B-2.4B-Writing-Thunder-V1.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "aquif-ai/aquif-3.5-8B-Think",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "aquif-ai/aquif-3.5-8B-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquif-ai/aquif-3.5-8B-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquif-ai/aquif-3.5-8B-Think",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M-Extract",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-RAG",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-Tool",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ring-mini-2.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ling-flash-2.0",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ring-mini-linear-2.0",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inclusionAI/Ring-flash-linear-2.0",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "small-models-for-glam/Qwen3-0.6B-SFT-name-parser-yaml",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "small-models-for-glam/Qwen3-0.6B-SFT-name-parser-yaml",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "gokaygokay/prompt-enhancer-gemma-3-270m-it",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gokaygokay/prompt-enhancer-gemma-3-270m-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "gokaygokay/prompt-enhancer-gemma-3-270m-it",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HarleyCooper/nanochat561",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/LunaMaid-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/LunaMaid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Vortex5/LunaMaid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/LunaMaid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-14B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-1.7B-3bit-SINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-1.7B-3bit-ASINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-1.7B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-14B-3bit-SINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-14B-3bit-ASINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-14B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-32B-3bit-SINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-SINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huawei-csl/Qwen3-32B-3bit-ASINQ",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huawei-csl/Qwen3-32B-3bit-ASINQ",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "EleutherAI/gpt-j-6b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-j-6b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "QuixiAI/WizardLM-7B-Uncensored",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "QuixiAI/WizardLM-7B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/WizardLM-7B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "QuixiAI/Wizard-Vicuna-13B-Uncensored",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-13b-chat-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lmsys/vicuna-7b-v1.5",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "codellama/CodeLlama-7b-Instruct-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/phi-1_5",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/phi-1_5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-6.7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-6.7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-6.7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-6.7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/phi-2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/phi-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/phi-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ZySec-AI/SecurityLLM",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "defog/sqlcoder-7b-2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "defog/sqlcoder-7b-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "defog/sqlcoder-7b-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-2b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "bigcode/starcoder2-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "sophosympatheia/Midnight-Miqu-70B-v1.5",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/codegemma-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/codegemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/codegemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/codegemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/codegemma-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "IlyaGusev/saiga_llama3_8b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "IlyaGusev/saiga_llama3_8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "IlyaGusev/saiga_llama3_8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "IlyaGusev/saiga_llama3_8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "IlyaGusev/saiga_llama3_8b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-3-mini-128k-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V2-Lite",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "dphn/dolphin-2.9.1-yi-1.5-34b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Sao10K/L3-70B-Euryale-v2.1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Sao10K/L3-70B-Euryale-v2.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-Coder-V2-Lite-Base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/wildguard",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/gemma-2-9b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "google/gemma-2-9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Groq/Llama-3-Groq-8B-Tool-Use",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Groq/Llama-3-Groq-8B-Tool-Use",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Groq/Llama-3-Groq-8B-Tool-Use",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Groq/Llama-3-Groq-8B-Tool-Use",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mlabonne/TwinLlama-3.1-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mlabonne/TwinLlama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "mlabonne/TwinLlama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mlabonne/TwinLlama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mlabonne/TwinLlama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mlabonne/TwinLlama-3.1-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Frowningface/Silly_Tavern_Presets_Database",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-3.5-mini-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-3B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inflatebot/MN-12B-Mag-Mell-R1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inflatebot/MN-12B-Mag-Mell-R1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "inflatebot/MN-12B-Mag-Mell-R1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inflatebot/MN-12B-Mag-Mell-R1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inflatebot/MN-12B-Mag-Mell-R1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inflatebot/MN-12B-Mag-Mell-R1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-72B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-32B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-3B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "unsloth/Llama-3.2-3B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/Llama-3.2-3B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "CohereLabs/aya-expanse-32b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM2-135M-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM2-360M-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-360M-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-360M-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nvidia/Hymba-1.5B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/Llama-3.1-Tulu-3-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/Llama-3.1-Tulu-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/Llama-3.1-Tulu-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "allenai/Llama-3.1-Tulu-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/Llama-3.1-Tulu-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/Llama-3.1-Tulu-3-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Llama-3.3-70B-Instruct-abliterated",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Llama-3.3-70B-Instruct-abliterated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Llama-3.3-70B-Instruct-abliterated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mlfoundations-dev/stackexchange_bioinformatics",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mlfoundations-dev/stackexchange_bioinformatics",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/Llama-3.1-Tulu-3-405B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/OLMoE-1B-7B-0125",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/OLMoE-1B-7B-0125",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/OLMoE-1B-7B-0125",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "m-a-p/YuE-s1-7B-anneal-en-cot",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/OLMoE-1B-7B-0125-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/OLMoE-1B-7B-0125-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/OLMoE-1B-7B-0125-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "SentientAGI/Dobby-Unhinged-Llama-3.3-70B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SentientAGI/Dobby-Unhinged-Llama-3.3-70B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-4-mini-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "GSAI-ML/LLaDA-8B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "yulan-team/YuLan-Mini-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "yulan-team/YuLan-Mini-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yulan-team/YuLan-Mini-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yulan-team/YuLan-Mini-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yulan-team/YuLan-Mini-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yulan-team/YuLan-Mini-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/OLMo-2-0325-32B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": null,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "allenai/OLMo-2-0325-32B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3-0324",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Gensyn/Qwen2.5-0.5B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gensyn/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gensyn/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gensyn/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gensyn/Qwen2.5-0.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-4-reasoning",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/Phi-4-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "microsoft/Phi-4-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/Phi-4-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-8B-Base",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "yamatazen/SnowElf-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/Phi-4-mini-reasoning",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "SWE-bench/SWE-agent-LM-32B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SWE-bench/SWE-agent-LM-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SWE-bench/SWE-agent-LM-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SWE-bench/SWE-agent-LM-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Qwen3-8B-abliterated",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Delta-Vector/Francois-PE-V2-Huali-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Delta-Vector/Francois-PE-V2-Huali-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Delta-Vector/Francois-PE-V2-Huali-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Delta-Vector/Francois-PE-V2-Huali-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Delta-Vector/Francois-PE-V2-Huali-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Delta-Vector/Francois-PE-V2-Huali-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Intelligent-Internet/II-Medical-8B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "wasmdashai/wasm-32B-Instruct-V1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "wasmdashai/wasm-32B-Instruct-V1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "wasmdashai/wasm-32B-Instruct-V1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "wasmdashai/wasm-32B-Instruct-V1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "yamatazen/FusionEngine-12B-Lorablated",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "opendatalab/meta-rater-1b-reasoning",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "opendatalab/meta-rater-1b-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "opendatalab/meta-rater-1b-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "opendatalab/meta-rater-1b-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "opendatalab/meta-rater-1b-reasoning",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/Flex-reddit-2x7B-1T",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Huihui-MoE-4.8B-A1.7B-abliterated",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Intelligent-Internet/II-Medical-8B-1706",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B-1706",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B-1706",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B-1706",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B-1706",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intelligent-Internet/II-Medical-8B-1706",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "MASARAT-SA/mubeen",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "agentica-org/DeepSWE-Preview",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "moonshotai/Kimi-K2-Base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Kwaipilot/KAT-V1-40B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kwaipilot/KAT-V1-40B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "PowerInfer/SmallThinker-4BA0.6B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepcogito/cogito-v2-preview-llama-70B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "unsloth/Qwen3-30B-A3B-Thinking-2507",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/Qwen3-30B-A3B-Thinking-2507",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "unsloth/gpt-oss-20b-BF16",
            "sft": {
                "fsdp": {
                    "is_supported": null,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/gpt-oss-20b-BF16",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "unsloth/gpt-oss-20b-BF16",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "unsloth/gpt-oss-20b-BF16",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "baichuan-inc/Baichuan-M2-32B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "baichuan-inc/Baichuan-M2-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "baichuan-inc/Baichuan-M2-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "baichuan-inc/Baichuan-M2-32B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "stepfun-ai/StepFun-Formalizer-7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stepfun-ai/StepFun-Formalizer-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "stepfun-ai/StepFun-Formalizer-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stepfun-ai/StepFun-Formalizer-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stepfun-ai/StepFun-Formalizer-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stepfun-ai/StepFun-Formalizer-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ByteDance-Seed/Seed-OSS-36B-Base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FreedomIntelligence/openPangu-Embedded-1B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FlareRebellion/WeirdCompound-v1.6-24b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "apple/FastVLM-0.5B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M-Math",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "OddTheGreat/Circuitry_24B_V.2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "swiss-ai/Apertus-70B-Instruct-2509",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "google/vaultgemma-1b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "aquigpt/open1-7.5B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "aquigpt/open1-7.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquigpt/open1-7.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquigpt/open1-7.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquigpt/open1-7.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aquigpt/open1-7.5B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "facebook/MobileLLM-R1-950M",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "GAIR/LIMI-Air",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "NhaiDao/Qwen3_IST",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "NhaiDao/Qwen3_IST",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "aoxo/gpt-oss-20b-uncensored",
            "sft": {
                "fsdp": {
                    "is_supported": null,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "aoxo/gpt-oss-20b-uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aoxo/gpt-oss-20b-uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "aoxo/gpt-oss-20b-uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Sunbird/Sunflower-14B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Kwaipilot/HiPO-8B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp-Base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "DreadPoor/Famino-12B-Model_Stock",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "orai-nlp/Gemma-Kimu-9b-it",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "orai-nlp/Gemma-Kimu-2b-it",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openmed-community/granite-4.0-micro-OpenMed",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openmed-community/granite-4.0-micro-OpenMed",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openmed-community/granite-4.0-micro-OpenMed",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nvidia/gpt-oss-120b-Eagle3-v2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "pyToshka/aws-security-analyst",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pyToshka/aws-security-analyst",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pyToshka/aws-security-analyst",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TECHNOPRAVIN01/Qwen2.5-3B-Valor",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-3B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-3B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TECHNOPRAVIN01/Qwen2.5-3B-Valor",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "unsloth/LFM2-8B-A1B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "turboderp/GLM-4.6-exl3-2.33bpw-opt",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nics-efc/C2C_Fuser",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "RESMP-DEV/GLM-4.6-NVFP4",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ordlibrary/SMOL",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "cesun/ReFIne-qwen3-1.7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cesun/ReFIne-qwen3-1.7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cesun/ReFIne-qwen3-1.7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cesun/ReFIne-qwen3-1.7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cesun/ReFIne-qwen3-1.7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "maywell/GLM-4.5-Air-GLM-4.6-Distill",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mlx-community/Ling-1T-mlx-3bit",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "blackbird/lfm2-vl-furigana-ocr",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "OpenBuddy/SimpleChat-72B-V4-Apache2.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "OpenBuddy/SimpleChat-72B-V4-Apache2.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/Dark-Quill-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Dark-Quill-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Dark-Quill-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Vortex5/Dark-Quill-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Dark-Quill-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Dark-Quill-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Clemylia/Lamina-suite-pretrain",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Dolphy-AI/Dolphy-1.0",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Dolphy-AI/Dolphy-1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Dolphy-AI/Dolphy-1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Dolphy-AI/Dolphy-1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Dolphy-AI/Dolphy-1.0",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "distil-labs/Distil-PII-Llama-3.2-1B-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "distil-labs/Distil-PII-Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "distil-labs/Distil-PII-Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "distil-labs/Distil-PII-Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "distil-labs/Distil-PII-Llama-3.2-1B-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "prithivMLmods/Logics-Qwen3-Math-4B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "prithivMLmods/Logics-Qwen3-Math-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "prithivMLmods/Logics-Qwen3-Math-4B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Thrillcrazyer/Qwen-1.5B_THIP",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Thrillcrazyer/Qwen-1.5B_THIP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Thrillcrazyer/Qwen-1.5B_THIP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Thrillcrazyer/Qwen-1.5B_THIP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Thrillcrazyer/Qwen-1.5B_THIP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Thrillcrazyer/Qwen-1.5B_THIP",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Lamapi/next-4b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Lamapi/next-4b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Lamapi/next-4b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Lamapi/next-4b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Lamapi/next-4b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Vortex5/Velvet-Orchid-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Velvet-Orchid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Vortex5/Velvet-Orchid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Vortex5/Velvet-Orchid-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "aquif-ai/aquif-4-Exp",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Clemylia/Dragoniland-SLM",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Keak-AI/keak-CRO-llama-3.1-8B-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Keak-AI/keak-CRO-llama-3.1-8B-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "SaptivaAI/KAL-24B-mx-v1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "SaptivaAI/KAL-24B-mx-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SaptivaAI/KAL-24B-mx-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SaptivaAI/KAL-24B-mx-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SaptivaAI/KAL-24B-mx-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "SaptivaAI/KAL-24B-mx-v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Retreatcost/VerbaMaxima-12B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Retreatcost/VerbaMaxima-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Retreatcost/VerbaMaxima-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Retreatcost/VerbaMaxima-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Retreatcost/VerbaMaxima-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Retreatcost/VerbaMaxima-12B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Clemylia/ABX",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ystemsrx/Qwen2.5-Sex",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ystemsrx/Qwen2.5-Sex",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ystemsrx/Qwen2.5-Sex",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ystemsrx/Qwen2.5-Sex",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ystemsrx/Qwen2.5-Sex",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openai-community/openai-gpt",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "EleutherAI/gpt-neo-1.3B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-neo-1.3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/gpt-neo-1.3B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "bolbolzaban/gpt2-persian",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lighteternal/gpt2-finetuned-greek",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/DialoGPT-medium",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/DialoGPT-medium",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/rugpt3medium_based_on_gpt2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "shibing624/macbert4csc-base-chinese",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "uer/gpt2-chinese-cluecorpussmall",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/mGPT",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "facebook/opt-1.3b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "bigscience/bloom-560m",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigscience/bloom-560m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigscience/bloom-560m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "bigscience/bloom-560m",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "NlpHUST/gpt2-vietnamese",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "bigscience/bloomz-560m",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/biogpt",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/biogpt",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/biogpt",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/biogpt",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/biogpt",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "stanford-crfm/BioMedLM",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stanford-crfm/BioMedLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "SRM47/gpt2-paraphraser",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/BioGPT-Large",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/BioGPT-Large",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "microsoft/BioGPT-Large-PubMedQA",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/BioGPT-Large-PubMedQA",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "microsoft/BioGPT-Large-PubMedQA",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "EleutherAI/pythia-6.9b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/pythia-6.9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/pythia-6.9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/pythia-6.9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "EleutherAI/pythia-6.9b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TurkuNLP/gpt3-finnish-large",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TurkuNLP/gpt3-finnish-large",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TurkuNLP/gpt3-finnish-large",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TurkuNLP/gpt3-finnish-large",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "TurkuNLP/gpt3-finnish-large",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "medalpaca/medalpaca-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "medalpaca/medalpaca-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "medalpaca/medalpaca-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "medalpaca/medalpaca-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "huggyllama/llama-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huggyllama/llama-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huggyllama/llama-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "huggyllama/llama-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huggyllama/llama-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "huggyllama/llama-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "yahma/llama-13b-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yahma/llama-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yahma/llama-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yahma/llama-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "yahma/llama-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "nomic-ai/gpt4all-j",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "nomic-ai/gpt4all-j",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "nomic-ai/gpt4all-j",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "nomic-ai/gpt4all-j",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "nomic-ai/gpt4all-j",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "nomic-ai/gpt4all-j",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lmsys/vicuna-13b-v1.1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "tiiuae/falcon-7b-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "shibing624/chinese-alpaca-plus-7b-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "shibing624/chinese-alpaca-plus-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "shibing624/chinese-alpaca-plus-7b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "pythainlp/adapter-wangchanglm-7.5B-sft-en-klongklon",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "QuixiAI/WizardLM-13B-Uncensored",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "coffeeee/nsfw-story-generator2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "roneneldan/TinyStories-1M",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "roneneldan/TinyStories-3M",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-3M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-3M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 2,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "sdpa",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-3M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "roneneldan/TinyStories-3M",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Kongfha/KlonSuphap-LM",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kongfha/KlonSuphap-LM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Kongfha/KlonSuphap-LM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openlm-research/open_llama_7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openlm-research/open_llama_3b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openlm-research/open_llama_3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mmaaz60/LLaVA-7B-Lightening-v1-1",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "javirandor/passgpt-10characters",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lmsys/vicuna-7b-v1.3",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "a686d380/rwkv-h-1b5",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lmsys/longchat-13b-16k",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/longchat-13b-16k",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/longchat-13b-16k",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/longchat-13b-16k",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/longchat-13b-16k",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "pankajmathur/orca_mini_v2_7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v2_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v2_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v2_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v2_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "thirupathibandam/autotrain-phanik-gpt-neo-125m-self-72606138970",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "thirupathibandam/autotrain-phanik-gpt-neo-125m-self-72606138970",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-13b-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "meta-llama/Llama-2-70b-chat-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-70b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 8
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "meta-llama/Llama-2-70b-chat-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Medlinker/Medgpt",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "lmsys/vicuna-13b-v1.5",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "matsuo-lab/weblab-10b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "matsuo-lab/weblab-10b-instruction-sft",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b-instruction-sft",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b-instruction-sft",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b-instruction-sft",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "matsuo-lab/weblab-10b-instruction-sft",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "pankajmathur/orca_mini_v3_7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v3_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v3_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v3_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v3_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "pankajmathur/orca_mini_v3_7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "togethercomputer/Llama-2-7B-32K-Instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "togethercomputer/Llama-2-7B-32K-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "togethercomputer/Llama-2-7B-32K-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "togethercomputer/Llama-2-7B-32K-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "togethercomputer/Llama-2-7B-32K-Instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "stabilityai/japanese-stablelm-base-alpha-7b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/mGPT-1.3B-bashkir",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-bashkir",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-bashkir",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/mGPT-1.3B-kazakh",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-kazakh",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-kazakh",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/mGPT-1.3B-tatar",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-tatar",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-tatar",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ai-forever/mGPT-1.3B-uzbek",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-uzbek",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ai-forever/mGPT-1.3B-uzbek",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Gryphe/MythoMax-L2-13b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "WizardLMTeam/WizardMath-70B-V1.0",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "PharMolix/BioMedGPT-LM-7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "PharMolix/BioMedGPT-LM-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "PharMolix/BioMedGPT-LM-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "PharMolix/BioMedGPT-LM-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "PharMolix/BioMedGPT-LM-7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Qwen/Qwen-VL",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "inceptionai/jais-13b-chat",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "inceptionai/jais-13b-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "sag-uniroma2/extremITA-Camoscio-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "sag-uniroma2/extremITA-Camoscio-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "sag-uniroma2/extremITA-Camoscio-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "sag-uniroma2/extremITA-Camoscio-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "sag-uniroma2/extremITA-Camoscio-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "codellama/CodeLlama-13b-Instruct-hf",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-7b-fast-instruct",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "EleutherAI/pile-t5-large",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "AdaptLLM/medicine-LLM",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "AdaptLLM/medicine-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/medicine-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/medicine-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "AdaptLLM/finance-LLM",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-LLM",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TheBloke/Llama-2-7B-Chat-AWQ",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "goendalf666/salesGPT_v2",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "mychen76/mistral7b_ocr_to_json_v1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "webbigdata/ALMA-7B-Ja",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "IlyaGusev/saiga_mistral_7b_lora",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "IlyaGusev/saiga_mistral_7b_lora",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "zai-org/agentlm-7b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "zai-org/agentlm-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "zai-org/agentlm-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "zai-org/agentlm-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "zai-org/agentlm-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "zai-org/agentlm-7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "stockmark/stockmark-13b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stockmark/stockmark-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stockmark/stockmark-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stockmark/stockmark-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stockmark/stockmark-13b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "webbigdata/ALMA-7B-Ja-V2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "webbigdata/ALMA-7B-Ja-V2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Skywork/Skywork-13B-base",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Skywork/Skywork-13B-base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Nondzu/Mistral-7B-codealpaca-lora",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-base",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "KSU-HW-SEC/Hardware_Phi_30k_version",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "CausalLM/14B-DPO-alpha",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "CausalLM/14B-DPO-alpha",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "CausalLM/14B-DPO-alpha",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "CausalLM/14B-DPO-alpha",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "CausalLM/14B-DPO-alpha",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TheBloke/deepseek-coder-6.7B-instruct-AWQ",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Pclanglais/MonadGPT",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Pclanglais/MonadGPT",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Pclanglais/MonadGPT",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Pclanglais/MonadGPT",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "DeepMount00/Mistral-Ita-7b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "FPHam/Generate_Question_Mistral_7B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FPHam/Generate_Question_Mistral_7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "FPHam/Generate_Question_Mistral_7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "FPHam/Generate_Question_Mistral_7B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "Intel/neural-chat-7b-v3-1",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "Intel/neural-chat-7b-v3-1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intel/neural-chat-7b-v3-1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intel/neural-chat-7b-v3-1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intel/neural-chat-7b-v3-1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "Intel/neural-chat-7b-v3-1",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "RWKV/rwkv-5-world-3b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "TheBloke/Writing_Partner_Mistral_7B-AWQ",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "BBuf/rwkv-5-world-1b5",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "KarlGauss/bert-base-italian-xxl-cased-finetuned-paisa",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "RevenueBase/mistral-7b-job-level-v15",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "RevenueBase/mistral-7b-job-level-v15",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "RevenueBase/mistral-7b-job-level-v15",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "RevenueBase/mistral-7b-job-level-v15",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "ceadar-ie/FinanceConnect-13B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ceadar-ie/FinanceConnect-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ceadar-ie/FinanceConnect-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "ceadar-ie/FinanceConnect-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ceadar-ie/FinanceConnect-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "ceadar-ie/FinanceConnect-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "tartuNLP/Llammas",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tartuNLP/Llammas",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "tartuNLP/Llammas",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tartuNLP/Llammas",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tartuNLP/Llammas",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "tartuNLP/Llammas",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "m-a-p/ChatMusician",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "m-a-p/ChatMusician",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/ChatMusician",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/ChatMusician",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/ChatMusician",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "m-a-p/ChatMusician",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "cosimoiaia/Loquace-7B-Mistral",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "AdaptLLM/finance-chat",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "cosimoiaia/Loquace-Wizard-13B",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "hedronstone/OpenHermes-7B-Symbolic",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "hedronstone/OpenHermes-7B-Symbolic",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "hedronstone/OpenHermes-7B-Symbolic",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "hedronstone/OpenHermes-7B-Symbolic",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "dphn/dolphin-2.5-mixtral-8x7b",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "dphn/dolphin-2.5-mixtral-8x7b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "VishalMysore/cookgptlama",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "VishalMysore/cookgptlama",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "cutycat2000/MeowGPT-2",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "cutycat2000/MeowGPT-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cutycat2000/MeowGPT-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cutycat2000/MeowGPT-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 2
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cutycat2000/MeowGPT-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "cutycat2000/MeowGPT-2",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "openchat/openchat-3.5-0106",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "autopipeline": {
                                "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                                "pp_microbatch_size": 1,
                                "pp_schedule": "interleaved1f1b",
                                "scale_grads_in_schedule": false
                            },
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "pp_size": 2,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "attn_implementation": "eager",
                                "pretrained_model_name_or_path": "openchat/openchat-3.5-0106",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true,
                                "use_cache": false,
                                "use_sdpa_patching": false
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 1024,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 2,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        },
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openchat/openchat-3.5-0106",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "openchat/openchat-3.5-0106",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "stabilityai/stable-code-3b",
            "sft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": true,
                    "validated_configs": [
                        {
                            "distributed": {
                                "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                                "cp_size": 1,
                                "dp_size": null,
                                "ep_size": 1,
                                "sequence_parallel": false,
                                "tp_size": 1
                            },
                            "loss_fn": {
                                "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                            },
                            "lr_scheduler": {
                                "lr_decay_style": "cosine",
                                "min_lr": 1e-06
                            },
                            "model": {
                                "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                                "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                                "torch_dtype": "bf16",
                                "trust_remote_code": true
                            },
                            "optimizer": {
                                "_target_": "torch.optim.adam.Adam",
                                "betas": [
                                    0.9,
                                    0.999
                                ],
                                "eps": 1e-08,
                                "lr": 1e-05,
                                "weight_decay": 0
                            },
                            "packed_sequence": {
                                "packed_sequence_size": 0,
                                "split_across_pack": false
                            },
                            "step_scheduler": {
                                "ckpt_every_steps": 50,
                                "global_batch_size": 16,
                                "local_batch_size": 1,
                                "max_steps": 100,
                                "val_every_steps": 2000
                            }
                        }
                    ]
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-moe-16b-chat",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        },
        {
            "model_id": "allenai/OLMo-7B",
            "sft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            },
            "peft": {
                "fsdp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "tp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "pp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "cp": {
                    "is_supported": false,
                    "validated_configs": []
                },
                "ep": {
                    "is_supported": false,
                    "validated_configs": []
                }
            }
        }
    ]
}