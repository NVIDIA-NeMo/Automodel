{
    "trendinglist_date": "20251009",
    "models": [
        {
            "model_id": "zai-org/GLM-4.6",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-small",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-small",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-micro",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ling-1T",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-8B-A1B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-tiny",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-micro",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ring-1T-preview",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ai21labs/AI21-Jamba-Reasoning-3B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ai21labs/AI21-Jamba-Reasoning-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai/gpt-oss-120b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-0.6B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai/gpt-oss-20b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openai/gpt-oss-20b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-micro-base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Salesforce/CoDA-v0-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-8B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-micro-base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TildeAI/TildeOpen-30b",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TildeAI/TildeOpen-30b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-Instruct-2507",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-tiny-base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-h-tiny-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-tiny-preview",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3n-E4B-it-litert-lm",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Kwaipilot/KAT-Dev",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Kwaipilot/KAT-Dev",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "moonshotai/Kimi-K2-Instruct-0905",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/bitnet-b1.58-2B-4T",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "jet-ai/Jet-Nemotron-4B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.6-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/Dolphin-Mistral-24B-Venice-Edition",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.5-Air",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-1B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "litert-community/Gemma3-1B-IT",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "swiss-ai/Apertus-8B-Instruct-2509",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-2.6B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-2.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-Extract",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3n-E2B-it-litert-lm",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai-community/gpt2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openai-community/gpt2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openai-community/gpt2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-Thinking-2507",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3-270m",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ring-mini-linear-2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ring-flash-linear-2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-1B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-1B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-3B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.3-70B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.3-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ling-flash-2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b-chat-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3-1b-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.5",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-Tool",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-Tool",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "facebook/MobileLLM-R1-950M",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "FractalAIResearch/Fathom-Search-4B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FractalAIResearch/Fathom-Search-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/Phi-3-mini-4k-instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "microsoft/Phi-3-mini-4k-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "microsoft/Phi-3-mini-4k-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-7B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-tiny-base-preview",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-base-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-base-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-4.0-tiny-base-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ERC-ITEA/MuduoLLM",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ERC-ITEA/MuduoLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "moonshotai/Kimi-K2-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B-Instruct-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "jet-ai/Jet-Nemotron-2B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "swiss-ai/Apertus-70B-Instruct-2509",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LLM360/K2-Think",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LLM360/K2-Think",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ring-flash-2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OpenGVLab/SDLM-32B-D4",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Tesslate/UIGENT-30B-3A-Preview",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Meta-Llama-3-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Meta-Llama-3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-2-2b-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2-2b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2-2b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-3B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-1.7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "suayptalha/Sungur-9B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM3-3B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM3-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Hermes-4-70B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "swiss-ai/Apertus-8B-2509",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-1.2B-RAG",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-1.2B-RAG",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.1-Terminus",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inferencerlabs/GLM-4.6-MLX-6.5bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/UserLM-8b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/UserLM-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Tesslate/UIGEN-FX-Agentic-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-Agentic-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.5-Air-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "FlareRebellion/WeirdCompound-v1.6-24b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "FlareRebellion/WeirdCompound-v1.6-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meituan-longcat/LongCat-Flash-Chat",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "swiss-ai/Apertus-70B-2509",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M-Extract",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Extract",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ring-mini-2.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 4,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 4,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "inclusionAI/Ring-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "GAIR/LIMI",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Salesforce/CoDA-v0-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Shekswess/trlm-135m",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Shekswess/trlm-135m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "foreverlasting1202/QuestA-Nemotron-1.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "foreverlasting1202/QuestA-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "fangwu97/DeepSearch-1.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "fangwu97/DeepSearch-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-SafeRL",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-4B-SafeRL",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "MikeRoz/GLM-4.6-exl3",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-2-7b-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-v0.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-2b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-0.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/phi-4",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/phi-4",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-3.3-8b-instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-8b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-0528",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/Phi-4-mini-flash-reasoning",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "astanahub/alemllm",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3-270m-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-270m-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-4B-Instruct-2507-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ByteDance-Seed/Seed-OSS-36B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-350M-Math",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-350M-Math",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Infinigence/Megrez2-3x7B-A3B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meituan-longcat/LongCat-Flash-Thinking",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3.2-Exp-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Vortex5/MN-14B-Crimson-Veil",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-14B-Crimson-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DreadPoor/Famino-12B-Model_Stock",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DreadPoor/Famino-12B-Model_Stock",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "YOYO-AI/Qwen3-30B-A3B-YOYO-V4",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AgentFlow/agentflow-planner-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ByteDance-Seed/BFS-Prover-V2-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "distilbert/distilgpt2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "distilbert/distilgpt2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "distilbert/distilgpt2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "bigscience/bloom",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-1.3b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2.9-llama3-8b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/Phi-3-mini-128k-instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "microsoft/Phi-3-mini-128k-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "microsoft/Phi-3-mini-128k-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "mlabonne/NeuralDaredevil-8B-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/NeuralDaredevil-8B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 64,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "HuggingFaceTB/SmolLM2-135M-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceTB/SmolLM2-135M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 128,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "m-a-p/YuE-s1-7B-anneal-en-cot",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "perplexity-ai/r1-1776",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/Phi-4-mini-instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "viettelsecurity-ai/security-llama3.2-3b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "viettelsecurity-ai/security-llama3.2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-V3-0324",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tiiuae/Falcon-E-3B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yamatazen/Gemma2-Snowflakes-9B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/Gemma2-Snowflakes-9B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-14B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "allenai/OLMo-2-0425-1B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "allenai/OLMo-2-0425-1B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yamatazen/FusionEngine-12B-Lorablated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/FusionEngine-12B-Lorablated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "moonshotai/Kimi-Dev-72B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "moonshotai/Kimi-Dev-72B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "marketeam/Qwen-Marketing",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Goedel-LM/Goedel-Prover-V2-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goedel-LM/Goedel-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.5-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-30B-A3B-Thinking-2507",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Jinx-org/Jinx-gpt-oss-20b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lmstudio-community/Qwen3-4B-Thinking-2507-MLX-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dousery/medical-reasoning-gpt-oss-20b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "apple/FastVLM-0.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "apple/FastVLM-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "driaforall/mem-agent",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "driaforall/mem-agent",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "driaforall/mem-agent",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "driaforall/mem-agent",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "driaforall/mem-agent",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "hokar3361/gpt-oss-coderjs-v0.1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ling-mini-2.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 4,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 4,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "inclusionAI/Ling-mini-2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "baidu/ERNIE-4.5-21B-A3B-Thinking",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "PerceptronAI/Isaac-0.1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "GeneralAnalysis/GA_Guard_Core",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Core",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Core",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Pinkstack/DistilGPT-OSS-qwen3-4B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Pinkstack/DistilGPT-OSS-qwen3-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lab-ii/Aina-14B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3Guard-Gen-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "RedHatAI/Qwen3-VL-235B-A22B-Instruct-FP8-dynamic",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OpenGVLab/SDLM-3B-D4",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OpenGVLab/SDLM-3B-D8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Tesslate/WEBGEN-Devstral-24B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/WEBGEN-Devstral-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-4.0-h-small-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-small",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-h-small",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuantTrio/GLM-4.6-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "onnx-community/granite-4.0-micro-ONNX-web",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "alpindale/GLM-4.6-INT8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-micro-unsloth-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-tiny",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-h-tiny",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-h-tiny",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-micro-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-small-FP8-Dynamic",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/Ring-mini-linear-2.0-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Vortex5/MN-12B-Azure-Veil",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/MN-12B-Azure-Veil",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Jackrong/gpt-oss-120b-Distill-Llama3.1-8B-v2",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Jackrong/gpt-oss-120b-Distill-Llama3.1-8B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Jackrong/gpt-oss-120b-Distill-Llama3.1-8B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-micro-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-granite-4.0-h-tiny-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-tiny-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-tiny-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yasserrmd/kallamni-4b-v1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yasserrmd/kallamni-4b-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "taozi555/MN-12B-Mag-Mell-R1-KTO",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "taozi555/MN-12B-Mag-Mell-R1-KTO",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai-community/gpt2-xl",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openai-community/gpt2-xl",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/DialoGPT-medium",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/DialoGPT-medium",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "facebook/opt-125m",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuixiAI/WizardLM-13B-Uncensored",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "QuixiAI/WizardLM-13B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/Llama-2-7B-Chat-GGML",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "JackFram/llama-68m",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JackFram/llama-68m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JackFram/llama-68m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Gryphe/MythoMax-L2-13b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gryphe/MythoMax-L2-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "codellama/CodeLlama-34b-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "epfl-llm/meditron-7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-llm-7b-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ZySec-AI/SecurityLLM",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ZySec-AI/SecurityLLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-7b-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-7b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "aaditya/Llama3-OpenBioLLM-70B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "aaditya/Llama3-OpenBioLLM-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "aaditya/Llama3-OpenBioLLM-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2.9.2-qwen2-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.2-qwen2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-70B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.1-70B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B-Instruct",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.1-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Hermes-3-Llama-3.1-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-3-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "utter-project/EuroLLM-1.7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "utter-project/EuroLLM-1.7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Hermes-3-Llama-3.1-405B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "HF1BitLLM/Llama3-8B-1.58-100B-tokens",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-72B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-72B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-32B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-32B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2.5-Coder-1.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2.5-Coder-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-3.2-3B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-3.2-3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-2-2b-jpn-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2-2b-jpn-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2-2b-jpn-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Hymba-1.5B-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "arcee-ai/Arcee-VyLinh",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "arcee-ai/Arcee-VyLinh",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Hymba-1.5B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "infly/OpenCoder-8B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "infly/OpenCoder-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yentinglin/Llama-3.1-Taiwan-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3.1-Taiwan-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "MaziyarPanahi/calme-3.2-instruct-78b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CohereLabs/c4ai-command-r7b-12-2024",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r7b-12-2024",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "jinaai/ReaderLM-v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jinaai/ReaderLM-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "sometimesanotion/Lamarck-14B-v0.7",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "sometimesanotion/Lamarck-14B-v0.7",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/MediPhi",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/MediPhi",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/MediPhi",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "microsoft/MediPhi",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/MediPhi",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/MediPhi",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "humain-ai/ALLaM-7B-Instruct-preview",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "humain-ai/ALLaM-7B-Instruct-preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-3-1b-pt",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-3-1b-pt",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/DeepSeek-V3-abliterated",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "BlinkDL/rwkv7-g1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ServiceNow-AI/Apriel-5B-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Salesforce/xLAM-2-3b-fc-r",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-3b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Salesforce/xLAM-2-1b-fc-r",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Salesforce/xLAM-2-1b-fc-r",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4-32B-0414",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "zai-org/GLM-4-32B-0414",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ibm-granite/granite-3.3-2b-instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ibm-granite/granite-3.3-2b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ServiceNow-AI/Apriel-5B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DeepHat/DeepHat-V1-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeepHat/DeepHat-V1-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-8B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-1.7B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3-1.7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yamatazen/SnowElf-12B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/SnowElf-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3-1.7B-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "microsoft/Phi-4-mini-reasoning",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "microsoft/Phi-4-mini-reasoning",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlabonne/Qwen3-14B-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mlabonne/Qwen3-14B-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/DeepSeek-Prover-V2-671B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Qwen3-4B-abliterated",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Gensyn/Qwen2.5-7B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Gensyn/Qwen2.5-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Qwen3-8B-abliterated",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "darkc0de/XortronCriminalComputingConfig",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/XortronCriminalComputingConfig",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tiiuae/Falcon-H1-7B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/Falcon-H1-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "darkc0de/BlackXorDolphTronGOAT",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "darkc0de/BlackXorDolphTronGOAT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yacht/Llama-3.2-1B-Instruct-CoreML",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yamatazen/NeonMaid-12B-v2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yamatazen/NeonMaid-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/NeonMaid-12B-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yamatazen/EsotericSage-12B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "yamatazen/EsotericSage-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Menlo/Jan-nano",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Menlo/Jan-nano",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Menlo/Jan-nano",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Menlo/Jan-nano",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "kyx0r/Neona-12B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kyx0r/Neona-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "agentica-org/DeepSWE-Preview",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "agentica-org/DeepSWE-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OddTheGreat/Mechanism_24B_V.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "OddTheGreat/Mechanism_24B_V.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "gustavecortal/Piaget-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gustavecortal/Piaget-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LiquidAI/LFM2-700M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-700M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LiquidAI/LFM2-700M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "LGAI-EXAONE/EXAONE-4.0-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "LGAI-EXAONE/EXAONE-4.0-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zai-org/GLM-4.5-FP8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OmniSVG/OmniSVG",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "zerofata/MS3.2-PaintedFantasy-v2-24B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "zerofata/MS3.2-PaintedFantasy-v2-24B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tencent/Hunyuan-7B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-8bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NVFP4/Qwen3-30B-A3B-Instruct-2507-FP4",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Hermes-4-405B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AmanPriyanshu/gpt-oss-10.8b-specialized-all-pruned-moe-only-15-experts",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AmanPriyanshu/gpt-oss-6.0b-specialized-harmful-pruned-moe-only-7-experts",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "kreasof-ai/Liquid-Thinking-Preview",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kreasof-ai/Liquid-Thinking-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kreasof-ai/Liquid-Thinking-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DavidAU/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Tesslate/UIGEN-FX-4B-Preview",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Tesslate/UIGEN-FX-4B-Preview",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/gpt-oss-20b-MXFP4-Q8",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "abocide/matchcommentary",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "abocide/Qwen2.5-7B-Instruct-R1-forfinance",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Hermes-4-14B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Hermes-4-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "OddTheGreat/Circuitry_24B_V.2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "OddTheGreat/Circuitry_24B_V.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cpatonn/NVIDIA-Nemotron-Nano-9B-v2-AWQ-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/vaultgemma-1b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Benyucong/rl_quantum_4b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Benyucong/rl_quantum_4b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "anikifoss/Kimi-K2-Instruct-0905-HQ4_K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/Qwen3-Next-80B-A3B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inclusionAI/Ling-flash-base-2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "GeneralAnalysis/GA_Guard_Lite",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Lite",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "GeneralAnalysis/GA_Guard_Lite",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "beyoru/Lunaa",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "beyoru/Lunaa",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "beyoru/Lunaa",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "beyoru/Lunaa",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "beyoru/Lunaa",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "aoxo/gpt-oss-20b-uncensored",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen3Guard-Gen-0.6B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen3Guard-Gen-0.6B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "suayptalha/Sungur-14B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "suayptalha/Sungur-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "haoranhe/ROVER-Qwen3-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "haoranhe/ROVER-Qwen3-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-GLM-4.5-Air-abliterated-mlx-mxfp4",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dongboklee/gPRM-14B-merged",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dongboklee/gPRM-14B-merged",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "DeryFerd/Qwen2.5-Math-Coder-Distill-Phi-2-4.4K-MixMathCode",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Vortex5/Harmonic-Moon-12B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/Harmonic-Moon-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Vortex5/Harmonic-Moon-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/Harmonic-Moon-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ByteDance-Seed/BFS-Prover-V2-32B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ByteDance-Seed/BFS-Prover-V2-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "PromptEnhancer/PromptEnhancer-Img2img-Edit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/GLM-4.6-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/GLM-4.6",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuantTrio/DeepSeek-V3.2-Exp-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AnuroopVJ/Adheren-phi4-finetuned",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AnuroopVJ/Adheren-phi4-finetuned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "GeneralAnalysis/GA_Guard_Thinking",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-micro",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-small-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/Apriel-1.5-15b-Thinker",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-micro",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-h-micro",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-micro-base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "unsloth/granite-4.0-micro-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/granite-4.0-h-micro-unsloth-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/Granite-4.0-H-Tiny-4bit-DWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/granite-4.0-h-tiny-3bit-MLX",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huihui-ai/Huihui-granite-4.0-h-micro-abliterated",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "purrgpt-community/Tiny-Purr-350M",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "purrgpt-community/Tiny-Purr-350M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AgentFlow/agentflow-planner-3b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AgentFlow/agentflow-planner-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Clemylia/Malya",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Clemylia/Malya",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Clemylia/Malya",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DavidAU/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "hoangtung386/TinyLlama-1.1B-qlora",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "hoangtung386/TinyLlama-1.1B-qlora",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "hoangtung386/TinyLlama-1.1B-qlora",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "hoangtung386/TinyLlama-1.1B-qlora",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Vortex5/Noir-Blossom-12B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/Noir-Blossom-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Vortex5/Noir-Blossom-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vortex5/Noir-Blossom-12B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mlx-community/LFM2-8B-A1B-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ysn-rfd/First_Persian_SLM_Big_Update_Version3_ULTIMATE_ysnrfd",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "turboderp/GLM-4.6-exl3-2.33bpw-opt",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Retreatcost/KansenSakura-Erosion-RP-12b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Retreatcost/KansenSakura-Erosion-RP-12b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Chun121/Qwen3-4B-RPG-Roleplay-V2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai-community/gpt2-large",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openai-community/gpt2-large",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai-community/gpt2-medium",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openai-community/openai-gpt",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Aleksandar1932/gpt2-spanish-classics",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "EleutherAI/gpt-neo-125m",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "EleutherAI/gpt-neo-125m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "EleutherAI/gpt-neo-125m",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "gabtan99/dialogpt-tagalog-medium-10",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gabtan99/dialogpt-tagalog-medium-10",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gabtan99/dialogpt-tagalog-medium-10",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "gagan3012/Fox-News-Generator",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "huggingtweets/porngum_ebooks",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggingtweets/porngum_ebooks",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nandinib1999/quote-generator",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nikokons/gpt2-greek",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nferruz/ProtGPT2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "shrishail/t5_paraphrase_msrp_paws",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "facebook/opt-1.3b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "succinctly/text2image-prompt-generator",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "KoboldAI/OPT-13B-Erebus",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "facebook/galactica-6.7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "PygmalionAI/pygmalion-6b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "PygmalionAI/pygmalion-6b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "huggyllama/llama-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "huggyllama/llama-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Neko-Institute-of-Science/OPT-175B-NumPy",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "vicgalle/gpt2-open-instruct-v1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "MetaIX/GPT4-X-Alpaca-30B-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "bigcode/starcoder",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tiiuae/falcon-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tiiuae/falcon-7b-instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tiiuae/falcon-7b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mosaicml/mpt-7b-instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "roneneldan/TinyStories-1M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "roneneldan/TinyStories-1Layer-21M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "roneneldan/TinyStories-1Layer-21M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "xyz-nlp/XuanYuan2.0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "QuixiAI/Wizard-Vicuna-30B-Uncensored",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-30B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-30B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "QuixiAI/Wizard-Vicuna-30B-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NousResearch/Nous-Hermes-13b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "NousResearch/Nous-Hermes-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/Nous-Hermes-13B-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CobraMamba/mamba-gpt-3b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CobraMamba/mamba-gpt-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cateto/korean-gpt-neox-125M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cateto/korean-gpt-neox-125M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lmsys/vicuna-7b-v1.3",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/airoboros-13B-gpt4-1.4-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/airoboros-13b-gpt4-1.4-SuperHOT-8K-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openlm-research/open_llama_7b_v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_7b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Maykeye/TinyLLama-v0",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Maykeye/TinyLLama-v0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Maykeye/TinyLLama-v0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Maykeye/TinyLLama-v0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-2-70b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/Llama-2-13b-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "meta-llama/Llama-2-13b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openlm-research/open_llama_3b_v2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openlm-research/open_llama_3b_v2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ilnikolaev/dvachGPT",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "georgesung/llama2_7b_chat_uncensored",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "georgesung/llama2_7b_chat_uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/llama2_7b_chat_uncensored-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lmsys/vicuna-7b-v1.5",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-7b-v1.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lmsys/vicuna-13b-v1.5-16k",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "lmsys/vicuna-13b-v1.5-16k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Universal-NER/UniNER-7B-all",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Universal-NER/UniNER-7B-all",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inceptionai/jais-13b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inceptionai/jais-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "inceptionai/jais-13b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen-VL",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "inceptionai/jais-13b-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "inceptionai/jais-13b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "codellama/CodeLlama-7b-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "codellama/CodeLlama-7b-Instruct-hf",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-7b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "codellama/CodeLlama-34b-Instruct-hf",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "codellama/CodeLlama-34b-Instruct-hf",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tiiuae/falcon-180B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "baichuan-inc/Baichuan2-7B-Chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "baichuan-inc/Baichuan2-7B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "baichuan-inc/Baichuan2-7B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/Falcon-180B-Chat-GPTQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nickypro/tinyllama-15M",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickypro/tinyllama-15M",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AdaptLLM/law-LLM",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/law-LLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AdaptLLM/law-LLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/law-LLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "klyang/MentaLLaMA-chat-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "klyang/MentaLLaMA-chat-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "thesephist/contra-bottleneck-t5-base-wikipedia",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mychen76/mistral7b_ocr_to_json_v1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mychen76/mistral7b_ocr_to_json_v1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "teknium/Mistral-Trismegistus-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "teknium/Mistral-Trismegistus-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Natooz/Maestro-REMI-bpe20k",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Nondzu/Mistral-7B-codealpaca-lora",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "HuggingFaceH4/zephyr-7b-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-coder-33b-base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "openchat/openchat_3.5",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openchat/openchat_3.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "openchat/openchat_3.5",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-coder-33b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "01-ai/Yi-34B-200K",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-34B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "01-ai/Yi-6B-200K",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "01-ai/Yi-6B-200K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "DeepMount00/Mistral-Ita-7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CodeTed/Chinese_Spelling_Correction_T5",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "GOAT-AI/GOAT-70B-Storytelling",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ethz-spylab/poisoned-rlhf-7b-SUDO-10",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "berkeley-nest/Starling-LM-7B-alpha",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "berkeley-nest/Starling-LM-7B-alpha",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "berkeley-nest/Starling-LM-7B-alpha",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "berkeley-nest/Starling-LM-7B-alpha",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "deepseek-ai/deepseek-llm-67b-chat",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "deepseek-ai/deepseek-llm-67b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nateraw/llama-2-7b-english-to-hinglish",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "bigcode/starcoder2-3b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cosimoiaia/Loquace-7B-Mistral",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AdaptLLM/finance-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/finance-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "AdaptLLM/law-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/law-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "AdaptLLM/law-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "AdaptLLM/law-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cosimoiaia/Loquace-Wizard-13B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cosimoiaia/Loquace-Wizard-13B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "upstage/SOLAR-10.7B-Instruct-v1.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-Instruct-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "upstage/SOLAR-10.7B-v1.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "upstage/SOLAR-10.7B-v1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Hypersniper/The_Philosopher_Zephyr_7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Hypersniper/The_Philosopher_Zephyr_7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2.5-mixtral-8x7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.5-mixtral-8x7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TriadParty/deepsex-6b-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "TriadParty/deepsex-6b-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "simecek/cswikimistral_0.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "simecek/cswikimistral_0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "simecek/cswikimistral_0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "simecek/cswikimistral_0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "simecek/cswikimistral_0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "WhiteRabbitNeo/WhiteRabbitNeo-13B-v1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2_6-phi-2",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "dphn/dolphin-2_6-phi-2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "elyza/ELYZA-japanese-Llama-2-13b-instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "jeiku/Rosa_NSFW_Niche_3B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jeiku/Rosa_NSFW_Niche_3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "jeiku/Rosa_NSFW_Niche_3B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "peiyi9979/math-shepherd-mistral-7b-prm",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "peiyi9979/math-shepherd-mistral-7b-prm",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "stabilityai/stable-code-3b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "stabilityai/stable-code-3b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "TheBloke/law-chat-AWQ",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Epiculous/Crunchy-onion",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen1.5-0.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen1.5-0.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "croissantllm/CroissantLLMChat-v0.1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "BioMistral/BioMistral-7B-AWQ-QGS128-W4-GEMM",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "tokyotech-llm/Swallow-MS-7b-v0.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tokyotech-llm/Swallow-MS-7b-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tokyotech-llm/Swallow-MS-7b-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "tokyotech-llm/Swallow-MS-7b-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "tokyotech-llm/Swallow-MS-7b-v0.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "M4-ai/TinyMistral-6x248M-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "musiclang/text-chord-predictor",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cais/HarmBench-Llama-2-13b-cls",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cais/HarmBench-Llama-2-13b-cls",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-2b-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/gemma-2b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "GritLM/GritLM-7B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "prometheus-eval/prometheus-7b-v2.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "prometheus-eval/prometheus-7b-v2.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "bigcode/starcoder2-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "bigcode/starcoder2-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "gordicaleksa/YugoGPT",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "gordicaleksa/YugoGPT",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Lishi0905/SoMeLVLM",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Lishi0905/SoMeLVLM",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-2b-cpp",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "abacusai/Liberated-Qwen1.5-72B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "state-spaces/mamba-130m-hf",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Uni-MoE/Uni-MoE-audio-e2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Uni-MoE/Uni-MoE-audio-base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "BUT-FIT/csmpt7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dominguesm/mambarim-110m",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CohereLabs/c4ai-command-r-v01",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/c4ai-command-r-v01",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "meta-llama/CodeLlama-7b-Python-hf",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Crystalcareai/GemMoE-Base-Random",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NexaAI/Octopus-v2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "xai-org/grok-1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-2b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-2b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-7b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-7b-it",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "google/codegemma-7b-it",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-2b-pytorch",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-7b-pytorch",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/codegemma-7b-it-pytorch",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "google/gemma-1.1-2b-it-tflite",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Nitral-AI/Nyanade_Stunna-Maid-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Nitral-AI/Nyanade_Stunna-Maid-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Nitral-AI/Nyanade_Stunna-Maid-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Nitral-AI/Nyanade_Stunna-Maid-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "apple/OpenELM-3B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/CodeQwen1.5-7B-Chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/CodeQwen1.5-7B-Chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nickmalhotra/ProjectIndus",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickmalhotra/ProjectIndus",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nickmalhotra/ProjectIndus",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/llama-3-8b-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "cgato/L3-TheSpice-8b-v0.1.3",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "cgato/L3-TheSpice-8b-v0.1.3",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "Vanessasml/cyber-risk-llama-3-8b",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Vanessasml/cyber-risk-llama-3-8b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "kuotient/Llama-3-6B-Instruct-pruned",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "kuotient/Llama-3-6B-Instruct-pruned",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Hannibal046/xrag-7b",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Snowflake/snowflake-arctic-base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NghiemAbe/Legal-Doc2Query",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2.9-llama3-8b-256k",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b-256k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b-256k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9-llama3-8b-256k",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "BramVanroy/fietje-2-chat",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "BramVanroy/fietje-2-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "BramVanroy/fietje-2-chat",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "NexaAI/octo-net",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "NexaAI/octo-net",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "NexaAI/octo-net",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "BAAI/Bunny-v1_0-4B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "BAAI/Bunny-v1_0-4B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "hjhj3168/Llama-3-8b-Orthogonalized-exl2",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "wdndev/tiny_llm_sft_92m",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "lelapa/InkubaLM-0.4B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "JetBrains/CodeLlama-7B-KStack",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "JetBrains/CodeLlama-7B-KStack",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "dphn/dolphin-2.9.1-yi-1.5-34b",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "dphn/dolphin-2.9.1-yi-1.5-34b",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CohereLabs/aya-23-8B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "CohereLabs/aya-23-8B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "CohereLabs/aya-23-35B",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "jinggu/jing-model",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "unsloth/mistral-7b-v0.3-bnb-4bit",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "yentinglin/Llama-3-Taiwan-70B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3-Taiwan-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "yentinglin/Llama-3-Taiwan-70B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "chincyk/PyCodeGen",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "chincyk/PyCodeGen",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2-0.5B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-0.5B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Writer/Palmyra-Med-70B-32K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Qwen/Qwen2-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Qwen/Qwen2-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "Sao10K/L3-8B-Stheno-v3.2",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "Sao10K/L3-8B-Stheno-v3.2",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "bosonai/Higgs-Llama-3-70B",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "bosonai/Higgs-Llama-3-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "bosonai/Higgs-Llama-3-70B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "mii-llm/maestrale-chat-v0.4-beta",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mii-llm/maestrale-chat-v0.4-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "mii-llm/maestrale-chat-v0.4-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "mii-llm/maestrale-chat-v0.4-beta",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "ajn313/cl-verilog-1.0",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "ajn313/cl-verilog-1.0",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "stefan-it/xlstm-german-wikipedia",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "rubra-ai/Qwen2-7B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "rubra-ai/Qwen2-7B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/OpenReasoning-Nemotron-1.5B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-1.5B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/OpenReasoning-Nemotron-7B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-7B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/OpenReasoning-Nemotron-14B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-14B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/OpenReasoning-Nemotron-32B",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/OpenReasoning-Nemotron-32B",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-47B-Reasoning-128K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-8B-Reasoning-128K",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Reasoning-128K",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-56B-Base-8K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-47B-Base-8K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-8B-Base-8K",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_replicate_size": 1,
                            "dp_size": null,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 1000,
                            "global_batch_size": 64,
                            "local_batch_size": 8,
                            "max_steps": 100,
                            "val_every_steps": 1000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-8B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-4B-Base-8K",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Base-8K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-H-4B-Instruct-128K",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "sdpa",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-H-4B-Instruct-128K",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Feedback",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.3-Nemotron-70B-Edit",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Edit",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.3-Nemotron-70B-Select",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.3-Nemotron-70B-Select",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-2M-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Mistral-NeMo-Minitron-8B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3_1-Nemotron-51B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-Mini-4B-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Nemotron-Mini-4B-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Minitron-4B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-4B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Minitron-8B-Base",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Minitron-8B-Base",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Minitron-4B-Base-En",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 8,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 2,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 2,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-8B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-2B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
            "sft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 2
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Mistral-NeMo-Minitron-4B-128K-Instruct",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/nemotron-3-8b-base-4k",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/nemotron-3-8b-chat-4k-sft",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/nemotron-3-8b-chat-4k-rlhf",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/nemotron-3-8b-chat-4k-steerlm",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/nemotron-3-8b-qa-4k",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 2,
                            "dp_size": null,
                            "ep_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 1,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
            "sft": {
                "fsdp": [],
                "tp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 200,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "num_epochs": 2,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 4,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 8,
                            "sequence_parallel": false,
                            "tp_size": 1
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 1024,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 256,
                            "local_batch_size": 32,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    },
                    {
                        "autopipeline": {
                            "_target_": "nemo_automodel.components.distributed.pipelining.autopipeline.AutoPipeline",
                            "pp_microbatch_size": 1,
                            "pp_schedule": "interleaved1f1b",
                            "scale_grads_in_schedule": false
                        },
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "activation_checkpointing": true,
                            "cp_size": 1,
                            "dp_size": null,
                            "ep_size": 1,
                            "pp_size": 4,
                            "sequence_parallel": false,
                            "tp_size": 8
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "attn_implementation": "eager",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true,
                            "use_cache": false,
                            "use_sdpa_patching": false
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 4096,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 16,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [
                    {
                        "distributed": {
                            "_target_": "nemo_automodel.components.distributed.fsdp2.FSDP2Manager",
                            "cp_size": 1,
                            "dp_size": null,
                            "pp_size": 1,
                            "sequence_parallel": false,
                            "tp_size": 4
                        },
                        "loss_fn": {
                            "_target_": "nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy"
                        },
                        "lr_scheduler": {
                            "lr_decay_style": "cosine",
                            "min_lr": 1e-06
                        },
                        "model": {
                            "_target_": "nemo_automodel.components._transformers.auto_model._BaseNeMoAutoModelClass.from_pretrained",
                            "pretrained_model_name_or_path": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
                            "torch_dtype": "bf16",
                            "trust_remote_code": true
                        },
                        "optimizer": {
                            "_target_": "torch.optim.adam.Adam",
                            "betas": [
                                0.9,
                                0.999
                            ],
                            "eps": 1e-08,
                            "lr": 1e-05,
                            "weight_decay": 0
                        },
                        "packed_sequence": {
                            "packed_sequence_size": 0,
                            "split_across_pack": false
                        },
                        "step_scheduler": {
                            "ckpt_every_steps": 50,
                            "global_batch_size": 32,
                            "local_batch_size": 4,
                            "max_steps": 100,
                            "val_every_steps": 2000
                        }
                    }
                ],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Llama-3.1-Nemotron-70B-Reward",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-4-340B-Instruct",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-4-340B-Reward",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        },
        {
            "model_id": "nvidia/Nemotron-4-340B-Base",
            "sft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            },
            "peft": {
                "fsdp": [],
                "tp": [],
                "pp": [],
                "cp": [],
                "ep": []
            }
        }
    ]
}