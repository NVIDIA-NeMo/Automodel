# Distributed Training

Distributed processing primitives for scaling training across multiple GPUs and nodes.

Browse the auto-generated documentation for distributed training modules:

- {doc}`../nemo_automodel/nemo_automodel.components.distributed.fsdp2` - FSDP2 implementation
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.ddp` - DDP implementation
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.nvfsdp` - nvFSDP implementation
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.cp_utils` - Context parallelism utilities
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.grad_utils` - Gradient utilities
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.init_utils` - Initialization utilities
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.optimized_tp_plans` - Tensor parallel plans
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.parallelizer` - Parallelizer
- {doc}`../nemo_automodel/nemo_automodel.components.distributed.tensor_utils` - Tensor utilities
