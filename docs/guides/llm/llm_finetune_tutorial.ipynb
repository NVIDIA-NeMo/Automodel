{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ NeMo AutoModel - LLM Fine-Tuning Tutorial\n",
        "\n",
        "This notebook demonstrates how to fine-tune Large Language Models (LLMs) using NeMo AutoModel.\n",
        "\n",
        "It implements the same functionality as:\n",
        "```bash\n",
        "python examples/llm_finetune/finetune.py --config examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml\n",
        "```\n",
        "\n",
        "## What you'll learn:\n",
        "1. How to load and customize configurations\n",
        "2. How to set up the training recipe\n",
        "3. How to run the training loop\n",
        "4. How to customize training parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the working directory to the repo root if running from notebooks folder\n",
        "REPO_ROOT = Path(\".\").resolve()\n",
        "if REPO_ROOT.name == \"notebooks\":\n",
        "    REPO_ROOT = REPO_ROOT.parent\n",
        "    os.chdir(REPO_ROOT)\n",
        "\n",
        "print(f\"Working directory: {REPO_ROOT}\")\n",
        "\n",
        "# Add repo to path if needed\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NeMo AutoModel imports\n",
        "from nemo_automodel.components.config.loader import load_yaml_config\n",
        "from nemo_automodel.recipes.llm.train_ft import TrainFinetuneRecipeForNextTokenPrediction\n",
        "\n",
        "print(\"‚úÖ NeMo AutoModel imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Configuration\n",
        "\n",
        "NeMo AutoModel uses YAML configuration files to define all training parameters.\n",
        "You can load a pre-defined config or create your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Load a pre-defined config from examples\n",
        "CONFIG_PATH = \"examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml\"\n",
        "\n",
        "# Load the YAML configuration\n",
        "cfg = load_yaml_config(CONFIG_PATH)\n",
        "\n",
        "print(f\"Loaded config from: {CONFIG_PATH}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Configuration Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Customize Configuration (Optional)\n",
        "\n",
        "You can modify any configuration parameter programmatically.\n",
        "This is equivalent to using `--key value` on the command line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Modify training parameters for a quick test run\n",
        "# Uncomment and modify as needed:\n",
        "\n",
        "# Change the model (use any HuggingFace model ID)\n",
        "# cfg.set_by_dotted(\"model.pretrained_model_name_or_path\", \"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "# Reduce epochs for testing\n",
        "# cfg.set_by_dotted(\"step_scheduler.num_epochs\", 1)\n",
        "\n",
        "# Change batch sizes\n",
        "# cfg.set_by_dotted(\"step_scheduler.global_batch_size\", 32)\n",
        "# cfg.set_by_dotted(\"step_scheduler.local_batch_size\", 4)\n",
        "\n",
        "# Change learning rate\n",
        "# cfg.set_by_dotted(\"optimizer.lr\", 1e-5)\n",
        "\n",
        "# Change validation frequency\n",
        "# cfg.set_by_dotted(\"step_scheduler.val_every_steps\", 50)\n",
        "\n",
        "# Change checkpoint frequency\n",
        "# cfg.set_by_dotted(\"step_scheduler.ckpt_every_steps\", 500)\n",
        "\n",
        "# Enable/disable torch.compile\n",
        "# cfg.set_by_dotted(\"compile.enabled\", True)\n",
        "\n",
        "print(\"Configuration customization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Quick Test Configuration\n",
        "\n",
        "For testing purposes, let's create a minimal configuration that runs quickly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test settings - uncomment to use\n",
        "QUICK_TEST = False  # Set to True for a quick test run\n",
        "\n",
        "if QUICK_TEST:\n",
        "    print(\"‚ö° Quick test mode enabled!\")\n",
        "    \n",
        "    # Reduce training to minimum\n",
        "    cfg.set_by_dotted(\"step_scheduler.num_epochs\", 1)\n",
        "    cfg.set_by_dotted(\"step_scheduler.global_batch_size\", 8)\n",
        "    cfg.set_by_dotted(\"step_scheduler.local_batch_size\", 2)\n",
        "    cfg.set_by_dotted(\"step_scheduler.val_every_steps\", 5)\n",
        "    cfg.set_by_dotted(\"step_scheduler.ckpt_every_steps\", 100)\n",
        "    \n",
        "    # Limit validation samples\n",
        "    cfg.set_by_dotted(\"validation_dataset.limit_dataset_samples\", 16)\n",
        "    \n",
        "    print(\"Quick test configuration applied!\")\n",
        "else:\n",
        "    print(\"Using full training configuration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. View Final Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the final configuration as a dictionary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Configuration:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Key settings\n",
        "print(f\"\\nüì¶ Model: {cfg.get('model.pretrained_model_name_or_path', 'N/A')}\")\n",
        "print(f\"üìä Dataset: {cfg.get('dataset.dataset_name', 'N/A')}\")\n",
        "print(f\"üîÑ Epochs: {cfg.get('step_scheduler.num_epochs', 'N/A')}\")\n",
        "print(f\"üìè Global Batch Size: {cfg.get('step_scheduler.global_batch_size', 'N/A')}\")\n",
        "print(f\"üìê Local Batch Size: {cfg.get('step_scheduler.local_batch_size', 'N/A')}\")\n",
        "print(f\"üìà Learning Rate: {cfg.get('optimizer.lr', 'N/A')}\")\n",
        "print(f\"‚úÖ Validation Every: {cfg.get('step_scheduler.val_every_steps', 'N/A')} steps\")\n",
        "print(f\"üíæ Checkpoint Every: {cfg.get('step_scheduler.ckpt_every_steps', 'N/A')} steps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize the Training Recipe\n",
        "\n",
        "The `TrainFinetuneRecipeForNextTokenPrediction` class orchestrates the entire training process:\n",
        "- Model loading and parallelization\n",
        "- Dataset and dataloader creation\n",
        "- Optimizer and scheduler setup\n",
        "- Checkpointing\n",
        "- Logging (WandB, MLflow, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the recipe instance\n",
        "recipe = TrainFinetuneRecipeForNextTokenPrediction(cfg)\n",
        "print(\"‚úÖ Recipe created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Setup the Recipe\n",
        "\n",
        "The `setup()` method initializes all components:\n",
        "- Distributed environment\n",
        "- Model and optimizer\n",
        "- Dataloaders\n",
        "- Schedulers\n",
        "- Checkpointer\n",
        "\n",
        "‚ö†Ô∏è **Note**: This step may download the model and dataset if not cached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup all components\n",
        "print(\"Setting up recipe components...\")\n",
        "print(\"This may take a few minutes on first run (downloading model/dataset)\\n\")\n",
        "\n",
        "recipe.setup()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Recipe setup complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inspect Recipe Components (Optional)\n",
        "\n",
        "After setup, you can inspect the initialized components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect model\n",
        "print(\"üìä Model Information:\")\n",
        "print(f\"  - Trainable Parameters: {recipe.param_info.get('trainable_params', 'N/A'):,}\")\n",
        "print(f\"  - Total Parameters: {recipe.param_info.get('total_params', 'N/A'):,}\")\n",
        "\n",
        "# Inspect dataloader\n",
        "print(f\"\\nüì¶ Dataloader:\")\n",
        "print(f\"  - Training batches: {len(recipe.dataloader)}\")\n",
        "print(f\"  - Validation datasets: {list(recipe.val_dataloaders.keys())}\")\n",
        "\n",
        "# Inspect step scheduler\n",
        "print(f\"\\n‚è±Ô∏è Training Schedule:\")\n",
        "print(f\"  - Total epochs: {recipe.step_scheduler.num_epochs}\")\n",
        "print(f\"  - Gradient accumulation steps: {recipe.step_scheduler.grad_acc_steps}\")\n",
        "print(f\"  - Checkpoint every: {recipe.step_scheduler.ckpt_every_steps} steps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run Training\n",
        "\n",
        "Execute the training loop. This will:\n",
        "1. Iterate through epochs and batches\n",
        "2. Perform forward/backward passes\n",
        "3. Update model parameters\n",
        "4. Run validation at specified intervals\n",
        "5. Save checkpoints at specified intervals\n",
        "\n",
        "‚ö†Ô∏è **Warning**: Training can take a long time depending on your configuration!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the training loop\n",
        "print(\"üèãÔ∏è Starting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "recipe.run_train_validation_loop()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Results\n",
        "\n",
        "After training completes, you can find:\n",
        "- **Checkpoints**: In the `checkpoints/` directory (or as configured)\n",
        "- **Training logs**: `training.jsonl` in the checkpoint directory\n",
        "- **Validation logs**: `validation.jsonl` in the checkpoint directory\n",
        "- **WandB/MLflow**: If configured, metrics are logged to these services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display checkpoint location\n",
        "checkpoint_dir = recipe.checkpointer.config.checkpoint_dir\n",
        "print(f\"üìÅ Checkpoints saved to: {checkpoint_dir}\")\n",
        "\n",
        "# List checkpoint contents\n",
        "import os\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"\\nCheckpoint directory contents:\")\n",
        "    for item in os.listdir(checkpoint_dir):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(\"\\n(Checkpoint directory not created yet)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Additional Examples\n",
        "\n",
        "### PEFT (LoRA) Fine-Tuning\n",
        "\n",
        "To use Parameter-Efficient Fine-Tuning with LoRA:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load a PEFT config\n",
        "# PEFT_CONFIG_PATH = \"examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml\"\n",
        "# cfg_peft = load_yaml_config(PEFT_CONFIG_PATH)\n",
        "# recipe_peft = TrainFinetuneRecipeForNextTokenPrediction(cfg_peft)\n",
        "# recipe_peft.setup()\n",
        "# recipe_peft.run_train_validation_loop()\n",
        "\n",
        "print(\"See examples/llm_finetune/ for more PEFT configurations!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Configuration from Scratch\n",
        "\n",
        "You can also create a configuration entirely in Python:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create config from scratch using ConfigNode\n",
        "from nemo_automodel.components.config.loader import ConfigNode\n",
        "\n",
        "custom_config_dict = {\n",
        "    \"step_scheduler\": {\n",
        "        \"global_batch_size\": 32,\n",
        "        \"local_batch_size\": 4,\n",
        "        \"ckpt_every_steps\": 500,\n",
        "        \"val_every_steps\": 50,\n",
        "        \"num_epochs\": 1,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"_target_\": \"nemo_automodel.NeMoAutoModelForCausalLM.from_pretrained\",\n",
        "        \"pretrained_model_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"_target_\": \"torch.optim.Adam\",\n",
        "        \"lr\": 1e-5,\n",
        "    },\n",
        "    # ... add other required sections\n",
        "}\n",
        "\n",
        "# To use: cfg_custom = ConfigNode(custom_config_dict)\n",
        "print(\"Custom configuration structure shown above!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîó Useful Links\n",
        "\n",
        "- [NeMo AutoModel Documentation](https://docs.nvidia.com/nemo/automodel/latest/index.html)\n",
        "- [GitHub Repository](https://github.com/NVIDIA-NeMo/Automodel)\n",
        "- [LLM Fine-tuning Examples](https://github.com/NVIDIA-NeMo/Automodel/tree/main/examples/llm_finetune)\n",
        "- [VLM Fine-tuning Examples](https://github.com/NVIDIA-NeMo/Automodel/tree/main/examples/vlm_finetune)\n",
        "\n",
        "## Available Config Files\n",
        "\n",
        "Check out `examples/llm_finetune/` for many pre-built configurations:\n",
        "- **Llama**: llama3_1, llama3_2, llama3_3\n",
        "- **Mistral**: mistral, mixtral\n",
        "- **Qwen**: qwen2.5, qwen3\n",
        "- **Gemma**: gemma, gemma2, gemma3\n",
        "- **Phi**: phi2, phi3, phi4\n",
        "- And many more!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
