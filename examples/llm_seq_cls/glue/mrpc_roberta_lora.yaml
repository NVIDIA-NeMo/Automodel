# GLUE MRPC with RoBERTa-large + LoRA

# To run this recipe:
#   automodel examples/llm_seq_cls/glue/mrpc_roberta_lora.yaml

recipe:
  _target_: nemo_automodel.recipes.llm.train_seq_cls.TrainFinetuneRecipeForSequenceClassification

step_scheduler:
  global_batch_size: 32
  local_batch_size: 32
  ckpt_every_steps: 200
  val_every_steps: 100
  num_epochs: 3
  max_steps: None

dist_env:
  backend: nccl
  timeout_minutes: 1

model:
  _target_: nemo_automodel.NeMoAutoModelForSequenceClassification.from_pretrained
  pretrained_model_name_or_path: roberta-large
  num_labels: 2

checkpoint:
  enabled: true
  checkpoint_dir: checkpoints/
  model_save_format: safetensors
  save_consolidated: true
  # Skip classifier head when loading base roberta-large (it doesn't have these layers)
  skip_task_head_prefixes_for_base_model: ["classifier."]

distributed:
  strategy: fsdp2
  dp_size: none
  tp_size: 1
  cp_size: 1

  sequence_parallel: false

peft:
  _target_: nemo_automodel.components._peft.lora.PeftConfig
  target_modules:
  - "*.query"
  - "*.value"
  # Note: classifier head is fully trained (not LoRA), unfrozen automatically in train_seq_cls.py
  dim: 8
  alpha: 16
  dropout: 0.1

dataset:
  _target_: nemo_automodel.components.datasets.llm.seq_cls.GLUE_MRPC
  split: train

dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater

validation_dataset:
  _target_: nemo_automodel.components.datasets.llm.seq_cls.GLUE_MRPC
  split: validation

validation_dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater

optimizer:
  _target_: torch.optim.AdamW
  betas: [0.9, 0.999]
  eps: 1e-8
  lr: 2.0e-5  # Standard learning rate for BERT/RoBERTa fine-tuning
  weight_decay: 0.01  # Crucial for stable training on small datasets

