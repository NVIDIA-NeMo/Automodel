{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Fine-Tune Qwen3-0.6B\n",
    "\n",
    "**What this notebook does:** installs NeMo AutoModel, fine-tunes a small LLM (20 steps), and runs inference on the result.\n",
    "\n",
    "**Requirements:** 1 NVIDIA GPU with 8+ GB VRAM (T4, L4, A10, or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -- Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nemo-automodel\n",
    "import nemo_automodel; print(f\"AutoModel ready: {nemo_automodel.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -- Fine-Tune (20 steps, ~2 min on T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo to get the example configs\n",
    "!git clone --depth 1 https://github.com/NVIDIA-NeMo/Automodel.git /tmp/automodel 2>/dev/null || true\n",
    "\n",
    "# Run fine-tuning: Qwen3-0.6B on HellaSwag, 20 steps\n",
    "!cd /tmp/automodel && automodel finetune llm \\\n",
    "  -c examples/llm_finetune/qwen/qwen3_0p6b_hellaswag.yaml \\\n",
    "  --checkpoint.enabled true \\\n",
    "  --checkpoint.model_save_format safetensors \\\n",
    "  --checkpoint.save_consolidated true \\\n",
    "  --step_scheduler.max_steps 20 \\\n",
    "  --step_scheduler.ckpt_every_steps 20 \\\n",
    "  --step_scheduler.val_every_steps 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 -- Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "ckpt = \"/tmp/automodel/checkpoints/epoch_0_step_20/model/consolidated/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 -- Use Your Own Data\n",
    "\n",
    "Replace the dataset section in the YAML config to use a local JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "\n",
    "# Create a tiny example dataset\n",
    "data = [\n",
    "    {\"instruction\": \"Translate 'Hello' to French\", \"output\": \"Bonjour\"},\n",
    "    {\"instruction\": \"What is the capital of Japan?\", \"output\": \"Tokyo\"},\n",
    "    {\"instruction\": \"Summarize photosynthesis in one sentence.\", \"output\": \"Plants convert sunlight into energy using chlorophyll.\"},\n",
    "]\n",
    "\n",
    "path = pathlib.Path(\"/tmp/my_data.jsonl\")\n",
    "path.write_text(\"\\n\".join(json.dumps(d) for d in data))\n",
    "print(f\"Wrote {len(data)} examples to {path}\")\n",
    "\n",
    "# To train on this data, update the YAML dataset section:\n",
    "yaml_snippet = \"\"\"\n",
    "dataset:\n",
    "  _target_: nemo_automodel.components.datasets.llm.column_mapped_text_instruction_dataset.ColumnMappedTextInstructionDataset\n",
    "  path_or_dataset_id: /tmp/my_data.jsonl\n",
    "  column_mapping:\n",
    "    question: instruction\n",
    "    answer: output\n",
    "  answer_only_loss_mask: true\n",
    "\"\"\"\n",
    "print(\"Add this to your YAML config:\")\n",
    "print(yaml_snippet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
