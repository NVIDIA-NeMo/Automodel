
step_scheduler:
  global_batch_size: 4
  local_batch_size: 2
  ckpt_every_steps: 10
  val_every_steps: 10
  num_epochs: 1
  max_steps: 10

dist_env:
  backend: nccl
  timeout_minutes: 1

rng:
  _target_: nemo_automodel.components.training.rng.StatefulRNG
  seed: 1111
  ranked: true

model:
  _target_: nemo_automodel.components.models.deepseek_v3.model.DeepseekV3ForCausalLM.from_config
  config:
    _target_: transformers.models.deepseek_v3.configuration_deepseek_v3.DeepseekV3Config
    vocab_size: 100
    hidden_size: 64
    intermediate_size: 128
    moe_intermediate_size: 128
    num_hidden_layers: 2
    num_attention_heads: 4
    num_key_value_heads: 4
    n_routed_experts: 4
    n_shared_experts: 1
    n_group: 1
    topk_group: 1
    num_experts_per_tok: 2
    routed_scaling_factor: 1.0
    first_k_dense_replace: 0
    norm_topk_prob: false
    torch_dtype: float32
  moe_config:
    _target_: nemo_automodel.components.moe.layers.MoEConfig
    n_routed_experts: 4
    n_shared_experts: 1
    n_activated_experts: 2
    n_expert_groups: 1
    n_limited_groups: 1
    train_gate: true
    gate_bias_update_factor: 0.001
    aux_loss_coeff: 0.0
    score_func: softmax
    route_scale: 1.0
    dim: 64
    inter_dim: 128
    moe_inter_dim: 128
    norm_topk_prob: false
    dtype: float32

peft:
  _target_: nemo_automodel.components._peft.lora.PeftConfig
  target_modules: ["*.experts"]
  dim: 4
  alpha: 8
  dropout: 0.0

checkpoint:
  enabled: false
  checkpoint_dir: checkpoints/
  model_save_format: safetensors
  load_base_model: false

distributed:
  _target_: nemo_automodel.components.distributed.fsdp2.FSDP2Manager
  dp_size: none
  dp_replicate_size: 1
  tp_size: 1
  cp_size: 1
  sequence_parallel: false

loss_fn:
  _target_: nemo_automodel.components.loss.masked_ce.MaskedCrossEntropy

dataset:
  _target_: nemo_automodel.components.datasets.llm.mock.build_unpacked_dataset
  num_sentences: 100
  vocab_size: 100
  max_sentence_len: 32

dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater
  shuffle: true

validation_dataset:
  _target_: nemo_automodel.components.datasets.llm.mock.build_unpacked_dataset
  num_sentences: 20
  vocab_size: 100
  max_sentence_len: 32

validation_dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  collate_fn: nemo_automodel.components.datasets.utils.default_collater

optimizer:
  _target_: torch.optim.Adam
  betas: [0.9, 0.999]
  eps: 1e-8
  lr: 1.0e-3
  weight_decay: 0
